171
Atomicity and anonymity are the two important requirements for application in electronic commerce. However absolutely anonymity may lead to confliction with law enforcement, e.g. blackmailing or money laundering. Therefore, it is important to design systems satisfying both atomicity and revocable anonymity. Based on the concept of two-phase commitment, we realize atomicity in electronic transaction with the Trusted Third Part as coordinator. Also we develops Brands' fair signature model, propose a method to enable not only anonymity but also owner-trace and money-trace.
Addressed in this paper is the issue of 'email data cleaning' for text mining. Many text mining applications need take emails as input. Email data is usually noisy and thus it is necessary to clean it before mining. Several products offer email cleaning features, however, the types of noises that can be eliminated are restricted. Despite the importance of the problem, email cleaning has received little attention in the research community. A thorough and systematic investigation on the issue is thus needed. In this paper, email cleaning is formalized as a problem of non-text filtering and text normalization. In this way, email cleaning becomes independent from any specific text mining processing. A cascaded approach is proposed, which cleans up an email in four passes including non-text filtering, paragraph normalization, sentence normalization, and word normalization. As far as we know, non-text filtering and paragraph normalization have not been investigated previously. Methods for performing the tasks on the basis of Support Vector Machines (SVM) have also been proposed in this paper. Features in the models have been defined. Experimental results indicate that the proposed SVM based methods can significantly outperform the baseline methods for email cleaning. The proposed method has been applied to term extraction, a typical text mining processing. Experimental results show that the accuracy of term extraction can be significantly improved by using the data cleaning method.
With the advent of the Semantic Web, there is a great need to upgrade existing web content to semantic web content. This can be accomplished through semantic annotations. Unfortunately, manual annotation is tedious, time consuming and error-prone. In this paper, we propose a tool, called iASA, that learns to automatically annotate web documents according to an ontology. iASA is based on the combination of information extraction (specifically, the Similarity-based Rule Learner—SRL) and machine learning techniques. Using linguistic knowledge and optimal dynamic window size, SRL produces annotation rules of better quality than comparable semantic annotation systems. Similarity-based learning efficiently reduces the search space by avoiding pseudo rule generalization. In the annotation phase, iASA exploits ontology knowledge to refine the annotation it proposes. Moreover, our annotation algorithm exploits machine learning methods to correctly select instances and to predict missing instances. Finally, iASA provides an explanation component that explains the nature of the learner and annotator to the user. Explanations can greatly help users understand the rule induction and annotation process, so that they can focus on correcting rules and annotations quickly. Experimental results show that iASA can reach high accuracy quickly.
Ontology mapping is the task of finding semantic relationships between entities (i.e. concept, attribute and relation) of two ontologies. In the existing literatures, many (semi-)automatic approaches have found considerable interest by combining several mapping strategies (namely multi-strategy mapping). However, experiments show that multi-strategy based mapping does not always outperform its single-strategy counterpart. We here mainly consider the following questions: For a new, unseen mapping task, should one use a multi-strategy or a single-strategy? And if the task is suitable for multi-strategy, then which strategies should be selected in the combined scenario? This paper proposes an approach of multiple strategies detection for ontology mapping. The results obtained so far show that multi-strategy detection improves both on precision and recall significantly.
A view validation algorithm has been shown to predict whether or not the views are sufficiently compatible for solving a particular learning task. But it only works when a natural split of features exists. If the split does not exist, it will fail to manufacture a feature split to build the best views. In this paper, we present a general algorithm CCFP (Correlation and Compatibility based Feature Partitioner) to automate multi-view detection. CCFP first labels the large amount of unlabeled examples using single view algorithm, then calculates the conditional SU (Symmetric Uncertainty) between every pair of features and the IG (Information Gain) of each feature given the examples labeled previously by single view algorithm with high-confidence predictions. According to the estimated values of SU and IG, all the features will be partitioned into two views that are low correlated, compatible and sufficient enough. The experiment results show that multi-view learner with views generated by CCFP outperforms learner with views generated by other means clearly.
This paper addresses the issue of ontology caching on semantic web. The Semantic Web is an extension of the current web in which information is given well-defined meaning, better enabling computers and people to work in cooperation. Ontology serves as the metadata for defining the information on semantic web. Ontology based semantic information retrieval (semantic retrieval) is becoming more and more important. Many research and industrial works have been made so far on semantic retrieval. Ontology based retrieval improves the performance of search engine and web mining. In semantic retrieval, a great number of accesses to ontologies usually lead the ontology servers to be very low efficient. To address this problem, it is indeed necessary to cache concepts and instances when ontology server is running. Existing caching methods from database community can be used in the ontology cache. However, they are not sufficient for dealing with the problem. In the task of caching in database, usually the most frequently accessed data are cached and the recently less frequently accessed data in the cache are removed from it. Different from that, in ontology base, data are organized as objects and relations between objects. User may request one object, and then request another object according to a relation of that object. He may also possibly request a similar object that has not any relations to the object. Ontology caching should consider more factors and is more difficult. In this paper, ontology caching is formalized as a problem of classification. In this way, ontology caching becomes independent from any specific semantic web application. An approach is proposed by using machine learning methods. When an object (e.g. concept or instance) is requested, we view its similar objects as candidates. A classification model is then used to predict whether each of these candidates should be cached or not. Features in classification models are defined. Experimental results indicate that the proposed methods can significantly outperform the baseline methods for ontology caching. The proposed method has been applied to a research project that is called SWARMS.
This paper is concerned with 'domain exploration'. By domain exploration, we mean searching for data on a specific domain which has been modeled by ontology. We have found that exploration needs on a domain can be categorized into data search and navigation, through an analysis of survey results. In our approach, the domain data is organized as instances of domain concepts. The instances are indexed and data search is conducted by index-based search which is also known the keyword based search. In data navigation, users can select an instance in the result of the index based search to display in visualization view. In visualization view, we use graph to visualize the instance and its context. By instance's context, we mean that other instances which has relations between the selected one. Users can click a node in the graph to navigate to its visualization view. In visualization view, we provide constraint-based search, users can input properties' values to search for instances that has the input value in the certain properties. Other than index-based search, we provide association search for any two instances. By association of two instances, we mean that the possible direct relations and indirect relations between the two instances. In this paper, we will describe SWARMS' architecture, component technologies and its performance in FOAF domain, because FOAF(Friend-Of-A-Friend) [1] domain's data size is largest among the three domains. The FOAF domain contains more than 90,000 persons and more than 100,000 publications of the people in the domain.
In this paper, we propose a new shape feature for shape-similarity search of 3D polygonal-mesh models in digital museum. The shape feature is an extension of the D2 shape functions proposed by Osada. Our proposed shape feature is a combination of geometry and texture which is also invariant to similarity transformation. Experiments showed that, our method achieved better performance improvement especially for appearance retrieval of 3d model.
Ontology mapping is the key point to reach interoperability over ontologies. In semantic web environment, ontologies are usually distributed and heterogeneous and thus it is necessary to find the mapping between them before processing across them. Many efforts have been conducted to automate the discovery of ontology mapping. However, some problems are still evident. In this paper, ontology mapping is formalized as a problem of decision making. In this way, discovery of optimal mapping is cast as finding the decision with minimal risk. An approach called Risk Minimization based Ontology Mapping (RiMOM) is proposed, which automates the process of discoveries on 1:1, n:1, 1:null and null:1 mappings. Based on the techniques of normalization and NLP, the problem of instance heterogeneity in ontology mapping is resolved to a certain extent. To deal with the problem of name conflict in mapping process, we use thesaurus and statistical technique. Experimental results indicate that the proposed method can significantly outperform the baseline methods, and also obtains improvement over the existing methods.
The large volume of web content needs to be annotated by ontologies (called Semantic Annotation), and our empirical study shows that strong dependencies exist across different types of information (it means that identification of one kind of information can be used for identifying the other kind of information). Conditional Random Fields (CRFs) are the state-of-the-art approaches for modeling the dependencies to do better annotation. However, as information on a Web page is not necessarily linearly laid-out, the previous linear-chain CRFs have their limitations in semantic annotation. This paper is concerned with semantic annotation on hierarchically dependent data (hierarch-ical semantic annotation). We propose a Tree-structured Conditional Random Field (TCRF) model to better incorporate dependencies across the hierarchic-ally laid-out information. Methods for performing the tasks of model-parameter estimation and annotation in TCRFs have been proposed. Experimental results indicate that the proposed TCRFs for hierarchical semantic annotation can significantly outperform the existing linear-chain CRF model.
Addressed in this paper is the issue of table extraction from plain text. Table is one of the commonest modes for presenting information. Table extraction has applications in information retrieval, knowledge acquisition, and text mining. Automatic information extraction from table is a challenge. Existing methods was mainly focusing on table extraction from web pages (formatted table extraction). So far the problem of table extraction on plain text, to the best of our knowledge, has not received sufficient attention. In this paper, unformatted table extraction is formalized as unformatted table block detection and unformatted table row identification. We concentrate particularly on the table extraction from Chinese documents. We propose to conduct the task of table extraction by combining machine learning methods and document structure. We first view the task as classification and propose a statistical approach to deal with it based on Naïve Bayes. We define features in the classification model. Next, we use document structure to improve the detection performance. Experimental results indicate that the proposed methods can significantly outperform the baseline methods for unformatted table extraction.
This paper is concerned with keyword extraction. By keyword extraction, we mean extracting a subset of words/phrases from a document that can describe the ‘meaning' of the document. Keywords are of benefit to many text mining applications. However, a large number of documents do not have keywords and thus it is necessary to assign keywords before enjoying the benefit from it. Several research efforts have been done on keyword extraction. These methods make use of the ‘global context information', which makes the performance of extraction restricted. A thorough and systematic investigation on the issue is thus needed. In this paper, we propose to make use of not only ‘global context information', but also ‘local context information' for extracting keywords from documents. As far as we know, utilizing both ‘global context information' and ‘local context information' in keyword extraction has not been sufficiently investigated previously. Methods for performing the tasks on the basis of Support Vector Machines have also been proposed in this paper. Features in the model have been defined. Experimental results indicate that the proposed SVM based method can significantly outperform the baseline methods for keyword extraction. The proposed method has been applied to document classification, a typical text mining processing. Experimental results show that the accuracy of document classification can be significantly improved by using the keyword extraction method.
This paper is concerned with the problem of semantic search. By semantic search, we mean searching for instances from knowledge base. Given a query, we are to retrieve ‘relevant’ instances, including those that contain the query keywords and those that do not contain the keywords. This is contrast to the traditional approaches of generating a ranked list of documents that contain the keywords. Specifically, we first employ keyword based search method to retrieve instances for a query; then a proposed method of semantic feedback is performed to refine the search results; and then we conduct re-retrieval by making use of relations and instance similarities. To make the search more effective, we use weighted ontology as the underlying data model in which importances are assigned to different concepts and relations. As far as we know, exploiting instance similarities in search on weighted ontology has not been investigated previously. For the task of instance similarity calculation, we exploit both concept hierarchy and properties. We applied our methods to a software domain. Empirical evaluation indicates that the proposed methods can improve the search performance significantly.
This paper addresses the issue of semantic annotation using horizontal and vertical contexts Semantic annotation is a task of annotating web pages with ontological information As information on a web page is usually two-dimensionally laid out, previous semantic annotation methods that view a web page as an ‘object' sequence have limitations In this paper, to better incorporate the two-dimensional contexts, semantic annotation is formalized as a problem of block detection and text annotation Block detection is aimed at detecting the text block by making use of context in one dimension and text annotation is aimed at detecting the ‘targeted instance' in the identified blocks using the other dimensional context A two-stage method for semantic annotation using machine learning has been proposed Experimental results indicate that the proposed method can significantly outperform the baseline method as well as the sequence-based method for semantic annotation.
Normal mesh is a new fundamental surface descrip-tion, which is multiresolution mesh where each level can be written as a normal offset from a coarser version. In this paper, we present an algorithm to create normal subdivision triangulations approxi-mating implicit surfaces. Our algorithm begins from a coarse base mesh created by a space-division based polygonization method for implicit surface, and the base mesh is then optimized by Laplacian smoothing operator. Subsequently, the faces of the base mesh are subdivided iteratively and the newly created vertices are located on the implicit surface along a certain direction, which depends on the normals of vertices of the faces. Finally, we obtain a piecewise-linear appro-ximation of the surface, which is a normal mesh with subdivision connectivity and provides a multi-resolution description of the implicit surface naturally. Under the same error restriction, the mesh has much fewer data with respect to the traditional polygoniza-tion algorithms.
This paper is concerned with the problem of name disambiguation. By name disambiguation, we mean distinguishing persons with the same name. It is a critical problem in many knowledge management applications. Despite much research work has been conducted, the problem is still not resolved and becomes even more serious, in particular with the popularity of Web 2.0. Previously, name disambiguation was often undertaken in either a supervised or unsupervised fashion. This paper first gives a constraint-based probabilistic model for semi-supervised name disambiguation. Specifically, we focus on investigating the problem in an academic researcher social network (http://arnetminer.org). The framework combines constraints and Euclidean distance learning, and allows the user to refine the disambiguation results. Experimental results on the researcher social network show that the proposed framework significantly outperforms the baseline method using unsupervised hierarchical clustering algorithm.
This paper addresses the issue of extraction of an academic researcher social network. By researcher social network extraction, we are aimed at finding, extracting, and fusing the `semantic'-based profiling information of a researcher from the Web. Previously, social network extraction was often undertaken separately in an ad-hoc fashion. This paper first gives a formalization of the entire problem. Specifically, it identifies the `relevant documents' from the Web by a classifier. It then proposes a unified approach to perform the researcher profiling using Conditional Random Fields (CRF). It integrates publications from the existing bibliography datasets. In the integration, it proposes a constraints-based probabilistic model to name disambiguation. Experimental results on an online system show that the unified approach to researcher profiling significantly outperforms the baseline methods of using rule learning or classification. Experimental results also indicate that our method to name disambiguation performs better than the baseline method using unsupervised learning. The methods have been applied to expert finding. Experiments show that the accuracy of expert finding can be significantly improved by using the proposed methods.
In actual databases, there are a lot of hierarchy coding data, existing clustering algorithms don't consider the special treatment of these data structure, so lead nonideal performance and clustering result. This paper proposes a new clustering algorithm to deal with the hierarchy coding data structure (HCDS) that exists in many applications. The main contributions include: (1) proposes a new concept for HCDS and corresponding definitions. (2) Proposes and implements a new clustering algorithm---CHCC (Coding Hierarchy Computing Based Clustering Algorithm) based on HCDS. (3) Proposes a fast algorithm for hierarchy coding structure processing. (4) Applies the algorithm into the clustering analysis of transient population for public security, and through extensive experiments, proves the validity and efficiency of the algorithm.
This paper proposes a novel algorithm of classification based on the similarities among data attributes. This method assumes data attributes of dataset as basic vectors of m dimensions, and each tuple of dataset as a sum vector of all the attribute-vectors. Based on transcendental concept similarity information among attributes, this paper suggests a novel distance algorithm to compute the similarity distance of each pairs of attribute-vectors. In the method, the computing of correlation is turned to attribute-vectors and formulas of their projections on each other, and the correlation among any two tuples of dataset can be worked out by computing these vectors and formulas. Based on the correlation computing method, this paper proposes a novel classification algorithm. Extensive experiments prove the efficiency of the algorithm.
In this paper, we present the design and implementation of our expertise oriented search system, EOS http://www.arnetminer.net. EOS is a researcher social network system. It has gathered information about a half-million computer science researchers from the Web and constructed a social network among the researchers through their co-authorship. In particular, the relationship in the social network information is used in both ranking experts for a given topic and searching for associations between researchers. Our experimental results demonstrate that the proposed methods for expert finding and association search in a social network are both more effective and efficient than the baseline methods.
This paper addresses the issue of researcher profiling. By researcher profiling, we mean building a semantic profile for an academic researcher, by identifying and annotating information from the Web. Previously, person profile annotation was often undertaken separately in an ad-hoc fashion. This paper first gives a formalization of the entire problem and proposes a unified approach to perform the task using Conditional Random Fields (CRF). The paper shows that with introduction of a set of tags, most of the annotation tasks can be performed within this approach. Experiments show that significant improvements over the separated method can be obtained, because the subtasks of annotation are interdependent and should be performed together. The method has been applied to expert finding. Experimental results show that the performance of expert finding can be significantly improved by using the profiling method.
Some applications in 3D mesh surface modeling, e.g. geologic surface reconstruction, require linear discontinuous mesh pieces reconstructed from input point cloud. Unfortunately as far as we know, those previously developed approaches failed in reconstructing such mesh directly from point cloud. This paper presents a new method which using mesh fitting to create discontinuity during geologic surface reconstruction. After improved Hoppe's reconstruction method, each vertex located at discontinuous area is adjusted to a new position based on quadric error metrics. Experiment results show that our method could generate high quality surfaces which satisfy both geometric and geologic constraints. Keyword: surface reconstruction; mesh fitting; quadric error metrics (QEM)
In this paper, we study the capacity of a large-scale random wireless network for multicast.Assume that n wireless nodes are randomly deployed in a square region with side-length a and all nodes have the uniform transmission range r and uniform interference range R r. We further assume that each wireless node can transmit/receive at W bits/second over a common wireless channel. For each node vi, we randomly pick k-1 nodes from the other n-1 nodes as the receivers of the multicast session rooted at node vi. The aggregated multicast capacity is defined as the total data rate of all multicast sessions in the network. In this paper we derive matching asymptotic upper bounds and lower bounds on multicast capacity of random wireless networks. We show that the total multicast capacity is Θ(√n over log n dot W over √ k) when k = Ω(n over log n); the total multicast capacity is Θ(W) when k =Θ(n over log n). Our bounds unify the previous capacity bounds on unicast (when k=2) by Gupta and Kumar [7]and the capacity bounds on broadcast (when k=n) in [11,20]. We also study the capacity of group-multicast for wireless networks where for each source node, we randomly select k-1 groups of nodes as receivers and the nodes in each group are within a constant hops from the group leader. The same asymptotic upper bounds and lower bounds still hold. For arbitrary networks, we provide a constructive lower bound Ω(√n over √k dot W) for aggregated multicast capacity when we can carefully place nodes and schedule node transmissions.
In this paper, we describe a Semantic Web application that builds a customizable conference calendar. In contrast to previous works aiming at manually creating a list of upcoming/current and past conferences, in this work we aim at providing a semantic conference calendar which automatically extracts information from the web using semantic annotation. In this system, to build a calendar, the user simply needs to specify what conferences he/she is interested in. The system finds, extracts, and updates the semantic information from the Web. We propose a unified approach for semantic annotation of the conference calendar. We also present evaluations of our approach on real-world data.
Program verification is a promising approach to improving program quality. To formally verify aspectoriented programs, we have to find a way to formally specify programs written in aspect-oriented languages. Pipa is a BISL tailored to AspectJ for specifying ...
Name ambiguity is a critical problem in many applications, in particular in the online bibliography systems, such as DBLP and CiteSeer. Previously, several clustering based methods have been proposed although, the problem still presents to be a big challenge for both research and industry communities. In this paper, we present a complementary study to the problem from another point of view. We propose an approach of finding atomic clusters to improve the performance of existing clustering-based methods. We conducted experiments on a dataset from a real-world system: Arnetminer.org. Experiments results show that significant improvements can be obtained by using the proposed atomic clusters finding approach (about +8% and +27% improvements depending on different clustering methods).
With the emergence of Information Communication Technology (ICT), enabling end users to take part in telecom service creation is critical for telecom operators. To stimulate the production of end user-generated services, the service platform should provide adaptation and personalization mechanisms to end users. We present a user profile repository approach that takes account of the personalization and adaptation of telecom service during creation and execution. It is shown how to interact with other components in order to enable creation, deployment and execution of services in a personalized way. We design the architecture of user profile repository, and implement a prototype to verify the functionality. We present how to use IETF XCAP (XML Configuration Access Protocol) protocol to handle user profiles data, and use SIP notification mechanism to subscribe user profiles data in the User Profile Repository.
Name ambiguity problem has been a challenging issue for a long history. In this paper, we intend to make a thorough investigation of the whole problem. Specifically, we formalize the name disambiguation problem in a unified framework. The framework can incorporate both attribute and relationship into a probabilistic model. We explore a dynamic approach for automatically estimating the person number K and employ an adaptive distance measure to estimate the distance between objects. Experimental results show that our proposed framework can significantly outperform the baseline method.
This paper addresses several key issues in extraction and mining of an academic social network: 1) extraction of a researcher social network from the existing Web; 2) integration of the publications from existing digital libraries; 3) expertise search on a given topic; and 4) association search between researchers. We developed a social network system, called ArnetMiner, based on proposed methods to the above problems. In total, 448,470 researcher profiles and 981,599 publications were extracted/integrated after the system having been in operation for two years. The paper describes the architecture and main features of the system. It also briefly presents the experimental results of the proposed methods.
This paper addresses several key issues in the ArnetMiner system, which aims at extracting and mining academic social networks. Specifically, the system focuses on: 1) Extracting researcher profiles automatically from the Web; 2) Integrating the publication data into the network from existing digital libraries; 3) Modeling the entire academic network; and 4) Providing search services for the academic network. So far, 448,470 researcher profiles have been extracted using a unified tagging approach. We integrate publications from online Web databases and propose a probabilistic framework to deal with the name ambiguity problem. Furthermore, we propose a unified modeling approach to simultaneously model topical aspects of papers, authors, and publication venues. Search services such as expertise search and people association search have been provided based on the modeling results. In this paper, we describe the architecture and main features of the system. We also present the empirical evaluation of the proposed methods.
In this paper, we propose a unified topic modeling approach and its integration into the random walk framework for academic search. Specifically, we present a topic model for simultaneously modeling papers, authors, and publication venues. We combine the proposed topic model into the random walk framework. Experimental results show that our proposed approach for academic search significantly outperforms the baseline methods of using BM25 and language model, and those of using the existing topic models (including pLSI, LDA, and the AT model).
With the Web content having been changed from homogeneity to heterogeneity, the recommendation becomes a more challenging issue. In this paper, we have investigated the recommendation problem on a general heterogeneous Web social network. We categorize the recommendation needs on it into two main scenarios: recommendation when a person is doing a search and recommendation when the person is browsing the information. We formalize the recommendation as a ranking problem over the heterogeneous network. Moreover, we propose using a random walk model to simultaneously ranking different types of objects and propose a pair-wise learning algorithm to learn the weight of each type of relationship in the model. Experimental results on two real-world data sets show that improvements can be obtained by comparing with the baseline methods.
This paper addresses the issue of identifying persons with expertise knowledge on a given topic. Traditional methods usually estimate the relevance between the query and the support documents of candidate experts using, for example, a language model. However, the language model lacks the ability of identifying semantic knowledge, thus results in some right experts cannot be found due to not occurrence of the query terms in the support documents. In this paper, we propose a mixture model based on Probabilistic Latent Semantic Analysis (PLSA) to estimate a hidden semantic theme layer between the terms and the support documents. The hidden themes are used to capture the semantic relevance between the query and the experts. We evaluate our mixture model in a real-world system, ArnetMiner. Experimental results indicate that the proposed model outperforms the language models.
In a directory ontology, only concepts and hypernym/hyponym relationships between them are defined. Directory ontologies are widely used in many real-world applications such as product catalogues and Web directories. The widely used heterogeneous directories also bring a big challenge for integration of them. Previously, little attentions have been paid attention to the problem in the research community. In this paper, we propose a path similarity based approach for directory ontology matching. We define the concept's local label and the concept's path label. Next, we propose a path similarity method, which combines the local label and path label information. Then, a top-down similarity flooding is utilized to improve the matching result. Our experimental results on the music genre ontologies show that the proposed method can achieve a precision of 78% and a recall of 66%, significantly outperforming the baseline method.
We address the problem of academic conference homepage understanding for the Semantic Web. This problem consists of three labeling tasks - labeling conference function pages, function blocks, and attributes. Different from traditional information extraction tasks, the data in academic conference homepages has complex structural dependencies across multiple Web pages. In addition, there are logical constraints in the data. In this paper, we propose a unified approach, Constrained Hierarchical Conditional Random Fields, to accomplish the three labeling tasks simultaneously. In this approach, complex structural dependencies can be well described. Also, the constrained Viterbi algorithm in the inference process can avoid logical errors. Experimental results on real world conference data have demonstrated that this approach performs better than cascaded labeling methods by 3.6% in F1-measure and that the constrained inference process can improve the accuracy by 14.3%. Based on the proposed approach, we develop a prototype system of use-oriented semantic academic conference calendar. The user simply needs to specify what conferences he/she is interested in. Subsequently, the system finds, extracts, and updates the semantic information from the Web, and then builds a calendar automatically for the user. The semantic conference data can be used in other applications, such as finding sponsors and finding experts. The proposed approach can be used in other information extraction tasks as well.
We study the multicast capacity of a random wireless network consisting of ordinary wireless nodes and base stations, known as a hybrid network. Assume that n ordinary wireless nodes are randomly deployed in a square region and all nodes have the uniform transmission range r and uniform interference range Rr. We further assume that each ordinary wireless node can transmit/receive at W bits/second over a common wireless channel. In addition, there are m additional base stations (neither source nodes nor receiver nodes) placed regularly in this square region and connected by a high-bandwidth wired network. For each ordinary node v, we randomly pick k-1 nodes from the other n-1 ordinary nodes as the receivers of the multicast session at node v. The aggregated multicast capacity is defined as the total data rate of all multicast sessions in this hybrid network. We derive asymptotic upper bounds and lower bounds on multicast capacity of the hybrid wireless networks. The total multicast capacity is O(√n /√log n · √m/k · W) when k = O(n / log n), k = O(m), k / √m → ∞ and m = o(a2 / r2); the total multicast capacity is Θ(√n / √log n · W / √k) when k = O(n/log n), k = Ω(m) and m/k → O. When k = O(n/log n) and k = O(√m), the upper bound for minimum multicast capacity is at most O(r·n/a · √m · W/k) and is at least Ω(W) respectively. When k =Ωα(n/log n), the multicast capacity is Θ(W).
This paper is concerned with the problem of expertise search in a time-varying social network. Previous research work on expertise search, aiming at finding the most important/authoritative objects, usually ignores an important factor - temporal information, which reveals a huge amount of information contained in large document collections. Many real-world applications, for example reviewers matching for academic papers and hot-topic finding from newsgroup posts need to consider the evolution of information over times. In this work, we propose a unified model by integrating the temporal information into a random walk model. Specifically, the time information is modelled in a forward-and-backward propagation process in the random walk. The proposed model has been applied to expertise search in an academic social network. Experimental results show that the proposed approach can significantly outperform the baseline methods of using the language model (2.0% in terms of MAP) and the traditional PageRank algorithm (17.2% in terms of MAP).
At our university, since the Spring of 2005, we have been teaching a first course about data cache on CMP from computer architecture. We have accomplished several goals. The most important of which is the analysis and experimental approach for pull-based data prefetching and push-based data prefetching on CMP. The pedagogical style embodied in this course fosters a good understanding of the relationship between memory wall and data prefetching for the students early in their experience.
Traditional ranking mainly focuses on one type of data source, and effective modeling still relies on a sufficiently large number of labeled or supervised examples. However, in many real-world applications, in particular with the rapid growth of the Web 2.0, ranking over multiple interrelated (heterogeneous) domains becomes a common situation, where in some domains we may have a large amount of training data while in some other domains we can only collect very little. One important question is: "if there is not sufficient supervision in the domain of interest, how could one borrow labeled information from a related but heterogenous domain to build an accurate model?". This paper explores such an approach by bridging two heterogeneous domains via the latent space. We propose a regularized framework to simultaneously minimize two loss functions corresponding to two related but different information sources, by mapping each domain onto a "shared latent space", capturing similar and transferable oncepts. We solve this problem by optimizing the convex upper bound of the non-continuous loss function and derive its generalization bound. Experimental results on three different genres of data sets demonstrate the effectiveness of the proposed approach.
We mainly study the achievable multicast throughput (AMT) for homogeneous wireless ad hoc networks under Gaussian Channel model. We focus on two typical random networks, i.e., random extended networks (REN) and random dense networks (RDN). In REN and RDN, n nodes are randomly distributed in the square region with side-length √n and 1, respectively. We randomly choose ns nodes as the sources of multicast sessions, and for each source v, we pick uniformly at random nd nodes as the destinations. We propose multicast schemes without using percolation theory, and analyze the achievable multicast throughput by taking account of all possible values of ns and nd. As a special case of our results, we show that for ns = Θ(n), the per-session AMT for RDN is Ω(1/√nd n log n) when nd = O(n/log n) and is Ω(1/n) when nd = Ω(n/log n); the per-session AMT for REN is Ω(1/√nd n ċ (log n)1-α/2) when nd = O(n/log n) and is Ω(1/nd ċ (log n)-α/2) when nd = Ω(n/log n), where α 2 denotes the power attenuation exponent.
In this paper, we study a novel problem of staring people discovery from social networks, which is concerned with finding people who are not only authoritative but also sociable in the social network. We formalize this problem as an optimization programming problem. Taking the co-author network as a case study, we define three objective functions and propose two methods to combine these objective functions. A genetic algorithm based method is further presented to solve this problem. Experimental results show that the proposed solution can effectively find the staring people from social networks.
The goal of query-focused summarization is to extract a summary for a given query from the document collection. Although much work has been done for this problem, there are still many challenging issues: (1) The length of the summary is predefined by, for example, the number of word tokens or the number of sentences. (2) A query usually asks for information of several perspectives (topics); however existing methods cannot capture topical aspects with respect to the query. In this paper, we propose a novel approach by combining statistical topic model and affinity propagation. Specifically, the topic model, called qLDA, can simultaneously model documents and the query. Moreover, the affinity propagation can automatically discover key sentences from the document collection without predefining the length of the summary. Experimental results on DUC05 and DUC06 data sets show that our approach is effective and the summarization performance is better than baseline methods.
Ontology matching plays a key role for semantic interoperability. Many methods have been proposed for automatically finding the alignment between heterogeneous ontologies. However, in many real-world applications, finding the alignment in a completely automatic way is highly infeasible . Ideally, an ontology matching system would have an interactive interface to allow users to provide feedbacks to guide the automatic algorithm. Fundamentally, we need answer the following questions: How can a system perform an efficiently interactive process with the user? How many interactions are sufficient for finding a more accurate matching? To address these questions, we propose an active learning framework for ontology matching, which tries to find the most informative candidate matches to query the user. The user's feedbacks are used to: 1) correct the mistake matching and 2) propagate the supervise information to help the entire matching process. Three measures are proposed to estimate the confidence of each matching candidate. A correct propagation algorithm is further proposed to maximize the spread of the user's "guidance". Experimental results on several public data sets show that the proposed approach can significantly improve the matching accuracy (+8.0% better than the baseline methods).
It is our great pleasure to welcome you to the 2nd ACM Workshop on Social Web Search & Mining -- SWSM'09. This year's workshop continues its tradition of being the premier forum for presentation of research results and experience reports on leading edge issues of searching and mining of the social web and social networks. The scope of the workshop includes, but not limited to web mining, social network analysis, semantic web, information retrieval, and natural language processing. Moreover, SWSM'09 plans to provide with delegates theoretical analyses as well as practical system perspectives on issues related to web search and mining. The call for papers attracted 17 submissions from Asia, Europe, and the United States. The program committee accepted 8 full papers and 1 short paper that cover a variety of topics, including social network modeling and analysis, social influence analysis, influence analysis, and semantic social network. In addition, the program a keynote speech by Dr. Ravi Kumar from Yahoo! Research. We hope that these proceedings will serve as a valuable reference to further the research in social network search and mining.
A folksonomy refers to a collection of user-defined tags with which users describe contents published on the Web. With the flourish of Web 2.0, folksonomies have become an important mean to develop the Semantic Web. Because tags in folksonomies are authored freely, there is a need to understand the structure and semantics of these tags in various applications. In this paper, we propose a learning approach to create an ontology that captures the hierarchical semantic structure of folksonomies. Our experimental results on two different genres of real world data sets show that our method can effectively learn the ontology structure from the folksonomies.
In large social networks, nodes (users, entities) are influenced by others for various reasons. For example, the colleagues have strong influence on one's work, while the friends have strong influence on one's daily life. How to differentiate the social influences from different angles(topics)? How to quantify the strength of those social influences? How to estimate the model on real large networks? To address these fundamental questions, we propose Topical Affinity Propagation (TAP) to model the topic-level social influence on large networks. In particular, TAP can take results of any topic modeling and the existing network structure to perform topic-level influence propagation. With the help of the influence analysis, we present several important applications on real data sets such as 1) what are the representative nodes on a given topic? 2) how to identify the social influences of neighboring nodes on a particular node? To scale to real large networks, TAP is designed with efficient distributed learning algorithms that is implemented and tested under the Map-Reduce framework. We further present the common characteristics of distributed learning algorithms for Map-Reduce. Finally, we demonstrate the effectiveness and efficiency of TAP on real large data sets.
In this paper, we present a study of a novel problem, i.e. topic-based citation recommendation, which involves recommending papers to be referred to. Traditionally, this problem is usually treated as an engineering issue and dealt with using heuristics. This paper gives a formalization of topic-based citation recommendation and proposes a discriminative approach to this problem. Specifically, it proposes a two-layer Restricted Boltzmann Machine model, called RBM-CS, which can discover topic distributions of paper content and citation relationship simultaneously. Experimental results demonstrate that RBM-CS can significantly outperform baseline methods for citation recommendation.
It is well known that Web users create links with different intentions. However, a key question, which is not well studied, is how to categorize the links and how to quantify the strength of the influence of a web page on another if there is a link between the two linked web pages. In this paper, we focus on the problem of link semantics analysis, and propose a novel supervised learning approach to build a model, based on a training link-labeled and link-weighted graph where a link-label represents the category of a link and a link-weight represents the influence of one web page on the other in a link. Based on the model built, we categorize links and quantify the influence of web pages on the others in a large graph in the same application domain. We discuss our proposed approach, namely Pairwise Restricted Boltzmann Machines (PRBMs), and conduct extensive experimental studies to demonstrate the effectiveness of our approach using large real datasets.
Ontology alignment identifies semantically matching entities in different ontologies. Various ontology alignment strategies have been proposed; however, few systems have explored how to automatically combine multiple strategies to improve the matching effectiveness. This paper presents a dynamic multistrategy ontology alignment framework, named RiMOM. The key insight in this framework is that similarity characteristics between ontologies may vary widely. We propose a systematic approach to quantitatively estimate the similarity characteristics for each alignment task and propose a strategy selection method to automatically combine the matching strategies based on two estimated factors. In the approach, we consider both textual and structural characteristics of ontologies. With RiMOM, we participated in the 2006 and 2007 campaigns of the Ontology Alignment Evaluation Initiative (OAEI). Our system is among the top three performers in benchmark data sets.
Motivated by the needs of precise forest inventory and real-time surveillance for ecosystem management, in this paper we present GreenOrbs [2], a wireless sensor network system and its application for canopy closure estimates. Both the hardware and software designs of GreenOrbs are tailored for sensing in wild environments without human supervision, including a firm weatherproof enclosure of sensor motes and a light-weight mechanism for node state monitoring and data collection. By incorporating a pre-deployment training process as well as a distributed calibration method, the estimates of canopy closure stay accurate and consistent against uncertain sensory data and dynamic environments. We have implemented a prototype system of GreenOrbs and carried out multiple rounds of deployments. The evaluation results demonstrate that GreenOrbs outperforms the conventional approaches for canopy closure estimates. Some early experiences are reported in this paper.
Knowledge of wetland use of migratory bird species during the annual life circle is important to construct conservation strategy and explore the implication for avian influenza control. Biological scientists have used GPS satellite telemetry to determine the habitat of wild birds. However, because there is not an efficient method to process the location data sets, scientists have to devote themselves to calculating the scattering location points in the GIS and using MCP or Kernel method to find the approximate home range of bird species. This paper presents an alternative hierarchy clustering-based methodology to build a Spatial-Tree for discovering the habitat area that overcomes the limit of human observation ability. Preliminary results showed that some nodes of the Spatial-Tree correspond to core range area providing an illusive map to depict the breeding and wintering home range of Bar-headed Goose.
Social bookmarking tools become more and more popular nowadays and tagging is used to organize information and allow users to recall or search the resources. Users need to type the tags whenever they post a resource, so that a good tag recommendation system can ease the process of finding some useful and relevant keywords for users. Researchers have made lots of relevant work for tag recommendation systems, but those traditional collaborative systems may not perform well in the case of a new user and a new resource. To address this problem, we propose a tag recommendation system for two different cases. The first case is that the active user or active resource is a new one which has not appeared in the past while the second case is that the active user and resource has already been posted. In this paper, we present two different methods for these two cases: a content-based method and a graph-based method. We implement our tag recommendation system in a realworld academic Folksonomy system. In order to gain better results,we apply improvements and combinations of previous work in each of these two methods.and combinations of previous work in each of these two methods. We participated into the Discovery Challenge 2009 with the proposed methods. The results of our teams are within the top teams.
Ontology matching, aiming to obtain semantic correspondences between two ontologies, has played a key role in data exchange, data integration and metadata management. Among numerous matching scenarios, especially the applications cross multiple domains, we observe an important problem, denoted as unbalanced ontology matching which requires to find the matches between an ontology describing a local domain knowledge and another ontology covering the information over multiple domains, is not well studied in the community. In this paper, we propose a novel Gauss Function based ontology matching approach to deal with this unbalanced ontology matching issue. Given a relative lightweight ontology which represents the local domain knowledge, we extract a "similar" sub-ontology from the corresponding heavyweight ontology and then carry out the matching procedure between this lightweight ontology and the newly generated sub-ontology. The sub-ontology generation is based on the influences between concepts in the heavyweight ontology. We propose a Gauss Function based method to properly calculate the influence values between concepts. In addition, we perform an extensive experiment to verify the effectiveness and efficiency of our proposed approach by using OAEI2007 tasks. Experimental results clearly demonstrate that our solution outperforms the existing methods in terms of precision, recall and elapsed time.
The DSTATCOM regulates the voltage at the point of common coupling (PCC) by injecting reactive power to the PCC, making it meet the requirement of the voltage quality. First, this paper deduces from the balance theory of instantaneous power of the DSTATCOM system the direct output voltage control strategy, in which the current detection circuit is not demanded. Compared with the cascade control strategy, it has the merits of a simple structure and fast response, but it doesn't perform well when the parameters of the system are changed. So the paper proposes the fuzzy PI control to solve the problem. The validity and effectiveness of the control strategy has been verified by theoretical analysis and digital simulation.
We present a middleware platform for assembling pervasive applications that demand fault-tolerance and adaptivity in distributed, dynamic environments. Unlike typical adaptive middleware approaches, in which sophisticated component model semantics are ...
Assume that n wireless nodes are randomly deployed in a square region with side-length a and all nodes have the uniform transmission range r and uniform interference range R = Θ(r). Each node is equipped with Φ interfaces. There are C = ∅(min(nr2=a2 log n)) channels of equal bandwidth W/C available. We consider a random (C g) channel assignment where each node may switch between a preassigned random subset of g channels (with g ≥ ∅Φ). In this paper, we study the multicast capacity of such a random wireless network, where for each node vi, we randomly pick k - 1 nodes from the other n - 1 nodes as the receivers of the multicast session rooted at node vi. We derive matching asymptotic upper bounds and lower bounds on multicast capacity. We show that the per-flow multicast capacity is Θ(W√Prnd/n log n ⋅ 1/√k) when k = ∅(Prnd⋅n/log n), where Prnd denotes the probability that two nodes share at least one channel. Our bounds unify the previous capacity bounds on unicast (when k = 2) by Bhandari and Vaidya [3] for multi-channel multi-radio networks.
This paper presents novel distributed algorithms for scheduling transmissions in multi-hop wireless networks. Our algorithms generate new schedules in a distributed manner via simple local changes to existing schedules. Two classes of algorithms are designed: one assumes that the location information of all wireless nodes are known, and the other does not. Both classes of algorithms are parameterized by an integer k (called algorithm-k). We show that algorithm-k that uses geometry location achieves (1 - 2/k)2 of the capacity region, for every k ≥ 3; algorithm-k which does not use geometry location achieves 1/ρ of the capacity region, for every k ≥ 3 and a constant ρ depending on k. Our algorithms have small worst-case overheads. Both classes of algorithms can generate a new schedule by requiring communications within Θ(k) hops for every node, which can be implemented by letting each node transmit at most O(k) messages. The parameter k explicitly captures the tradeoff between control overhead and the throughput performance of any scheduler. Additionally, the class of algorithms with known geometry location of nodes can find a new schedule in time Θ(k2 Δ), where Δ is the minimum mini-time-slots such that each of the n nodes can communicate with its neighbors once, which is the minimum time-slots required by any scheduling algorithm.
We study the capacity for both random and arbitrary wireless networks under Gaussian Channel model when all wireless nodes have the same constant transmission power P. During the transmission, the power decays along path with attenuation exponent β 2. We consider extended networks, where n wireless nodes {v1, v2,..., vn} are randomly or arbitrarily distributed in a square region Ba with side-length a. We randomly choose ns multicast sessions. For each source node vi, we randomly select k points pi,j (1 ≤ j ≤ k) in Ba and the node which is closest to pi,j will serve as a destination node of vi. We derive the achievable upper bounds on unicast capacity and an upper bound (partially achievable) on multicast capacity of the wireless networks under Gaussian Channel model. We found that the unicast (multicast) capacity for wireless networks under Gaussian Channel model has three regimes.
The temperature and humidity decoupling control method was presented in this paper to resolve the coupling problem between temperature and humidify in central air-conditioning variable dew point control system. By introducing absolute humidity as an intermediate variable to control the machine dew point, temperature and humidify of system can be decoupled. Two input and two output transfer function model was established by combining the theoretical analysis with the data analysis of experiment,and a control system consisting of a feed forward compensation decouple control and PID control was designed. The simulation based on MATLAB results demonstrated that this method is effective,and is a1so effective to control temperature and humidity system.
Link scheduling is crucial in improving the throughput in wireless networks and it has been widely studied under various interference models. In this paper, we study the link scheduling problem under physical interference model where all senders of the links transmit at a given power P and a link can transmit successfully if and only if the Signal-to-Interference-plus-Noise-Ratio (SINR) at the corresponding receiver is at least a certain threshold. The link scheduling problem is to find a maximum "independent set" (MIS) of links, i.e., the maximum number of links that can transmit successfully in one time-slot, given a set of input links. This problem has been shown to be NP-hard [10]. Here we propose the first link scheduling algorithm with a constant approximation ratio for arbitrary background noise N ≥ 0. When each link l has a weight w(l)0, we propose a method for weighted MIS with approximation ratio O(min(log maxι∈L w(ι)/minι∈L w||ι||, log maxι∈L w(ι)/minι∈L w||ι||, where ||ι|| is the Euclidean length of a link ι.
Data aggregation is a primitive communication task in wireless sensor networks (WSNs). In this paper, we study designing data aggregation schedules under the Protocol Interference Model for answering queries. Given a network consisting of a set of nodes V distributed in a two-dimensional plane, we address different kinds of queries in this paper. First and foremost, we consider a single one-off query which requires a subset of source nodes V′ ⊆ V to send data to a distinguished sink node, we propose a delay-efficient algorithm that produces a collision-free schedule and theoretically prove that the delay achieved by our algorithm is nearly a small constant factor of the optimum. We further extend our discussion to the multiple one-off queries case and periodic query case and propose our data aggregation scheduling algorithms respectively with theoretical performance analysis.
Data aggregation is an efficient primitive in wireless sensor network (WSN) applications. This paper focuses on data aggregation scheduling problem to minimize the latency. We propose an efficient distributed method that produces a collision-free schedule for data aggregation in WSNs. We prove that the latency of the aggregation schedule generated by our algorithm is at most 16R+Δ--14 time-slots. Here R is the network radius and Δ is the maximum node degree in the communication graph of the original network. Our method significantly improves the previously known best data aggregation algorithm [3], that has a latency bound of 24D+6Δ+16 time-slots, where D is the network diameter (Note that D can be as large as 2R). We conduct extensive simulations to study the practical performances of our proposed data aggregation method. Our simulation results corroborate our theoretical results and show that our algorithms perform better in practice. We prove that the overall lower-bound of latency of data aggregation under any interference model is max{log n, R} where n is the network size. We provide an example to show that the lower-bound is (approximately) tight under protocol interference model when rI=r, where rI is the interference range and r is the transmission range. We also derive the lower-bound of latency under protocol interference model when r rI r and rI ≥ 3r.
In this paper, we study the problem of topic-level random walk, which concerns the random walk at the topic level. Previously, several related works such as topic sensitive page rank have been conducted. However, topics in these methods were predefined, which makes the methods inapplicable to different domains. In this paper, we propose a four-step approach for topic-level random walk. We employ a probabilistic topic model to automatically extract topics from documents. Then we perform the random walk at the topic level. We also propose an approach to model topics of the query and then combine the random walk ranking score with the relevance score based on the modeling results. Experimental results on a real-world data set show that our proposed approach can significantly outperform the baseline methods of using language model and that of using traditional PageRank.
It is well known that users' behaviors (actions) in a social network are influenced by various factors such as personal interests, social influence, and global trends. However, few publications systematically study how social actions evolve in a dynamic social network and to what extent different factors affect the user actions. In this paper, we propose a Noise Tolerant Time-varying Factor Graph Model (NTT-FGM) for modeling and predicting social actions. NTT-FGM simultaneously models social network structure, user attributes and user action history for better prediction of the users' future actions. More specifically, a user's action at time t is generated by her latent state at t, which is influenced by her attributes, her own latent state at time t-1 and her neighbors' states at time t and t-1. Based on this intuition, we formalize the social action tracking problem using the NTT-FGM model; then present an efficient algorithm to learn the model, by combining the ideas from both continuous linear system and Markov random field. Finally, we present a case study of our model on predicting future social actions. We validate the model on three different types of real-world data sets. Qualitatively, our model can uncover some interesting patterns of the social dynamics. Quantitatively, experimental results show that the proposed method outperforms several baseline methods for action prediction.
Information network contains abundant knowledge about relationships among people or entities. Unfortunately, such kind of knowledge is often hidden in a network where different kinds of relationships are not explicitly categorized. For example, in a research publication network, the advisor-advisee relationships among researchers are hidden in the coauthor network. Discovery of those relationships can benefit many interesting applications such as expert finding and research community analysis. In this paper, we take a computer science bibliographic network as an example, to analyze the roles of authors and to discover the likely advisor-advisee relationships. In particular, we propose a time-constrained probabilistic factor graph model (TPFG), which takes a research publication network as input and models the advisor-advisee relationship mining problem using a jointly likelihood objective function. We further design an efficient learning algorithm to optimize the objective function. Based on that our model suggests and ranks probable advisors for every author. Experimental results show that the proposed approach infer advisor-advisee relationships efficiently and achieves a state-of-the-art accuracy (80-90%). We also apply the discovered advisor-advisee relationships to bole search, a specific expert finding task and empirical study shows that the search performance can be effectively improved (+4.09% by NDCG@5).
Exploring community is fundamental for uncovering the connections between structure and function of complex networks and for practical applications in many disciplines such as biology and sociology. In this paper, we propose a TTR-LDA-Community model which combines the Latent Dirichlet Allocation model (LDA) and the Girvan-Newman community detection algorithm with an inference mechanism. The model is then applied to data from Delicious, a popular social tagging system, over the time period of 2005-2008. Our results show that 1) users in the same community tend to be interested in similar set of topics in all time periods; and 2) topics may divide into several sub-topics and scatter into different communities over time. We evaluate the effectiveness of our model and show that the TTR-LDA-Community model is meaningful for understanding communities and outperforms TTR-LDA and LDA models in tag prediction.
Statistical topic models have been proposed for modeling documents and authorship information. However, few previous works have studied the evolution of associated data. In this paper, we investigate how to model trends of changes in document content and author interests simultaneously over time. We propose two models: a bag-of-words based Author-Time-Topic model that extends the state-of-the-art LDA-style topic model and a Hidden Markov Author-Time-Topic model, which can model interdependencies between topics. We use the Gibbs EM algorithm for parameter estimation. We apply these models to two data sets: NIPS papers and Yahoo group posts. Experimental results show that our models can achieve a lower perplexity (-2.0%-20%) than the baseline LDA and Author-Topic model, when modeling quickly evolving associated data. Experiments also reveal that the proposed models can accurately capture the hot topics in different periods (e.g. ''Yao at preseason'' in Aug-2004, when the Chinese player Ming Yao became a highlight in the NBA) from the two data sets.
In this article, we study the problem of Web user profiling, which is aimed at finding, extracting, and fusing the “semantic”-based user profile from the Web. Previously, Web user profiling was often undertaken by creating a list of keywords for the user, which is (sometimes even highly) insufficient for main applications. This article formalizes the profiling problem as several subtasks: profile extraction, profile integration, and user interest discovery. We propose a combination approach to deal with the profiling tasks. Specifically, we employ a classification model to identify relevant documents for a user from the Web and propose a Tree-Structured Conditional Random Fields (TCRF) to extract the profile information from the identified documents; we propose a unified probabilistic model to deal with the name ambiguity problem (several users with the same name) when integrating the profile information extracted from different sources; finally, we use a probabilistic topic model to model the extracted user profiles, and construct the user interest model. Experimental results on an online system show that the combination approach to different profiling tasks clearly outperforms several baseline methods. The extracted profiles have been applied to expert finding, an important application on the Web. Experiments show that the accuracy of expert finding can be improved (ranging from +6&percnt; to +26&percnt; in terms of MAP) by taking advantage of the profiles.
Establishing trust amongst agents is of central importance to the development of well-functioning multi-agent systems. For example, the anonymity of transactions on the Internet can lead to inefficiencies; e.g., a seller on eBay failing to ship a good as promised, or a user free-riding on a file-sharing network. Trust (or reputation) mechanisms can help by aggregating and sharing trust information between agents. Unfortunately these mechanisms can often be manipulated by strategic agents. Existing mechanisms are either very robust to manipulation (i.e., manipulations are not beneficial for strategic agents), or they are very informative (i.e., good at aggregating trust data), but never both. This paper explores this trade-off between these competing desiderata. First, we introduce a metric to evaluate the informativeness of existing trust mechanisms. We then show analytically that trust mechanisms can be combined to generate new hybrid mechanisms with intermediate robustness properties. We establish through simulation that hybrid mechanisms can achieve higher overall efficiency in environments with risky transactions and mixtures of agent types (some cooperative, some malicious, and some strategic) than any previously known mechanism.
In this paper, we study a novel problem Collective Active Learning, in which we aim to select a batch set of "informative" instances from a networking data set to query the user in order to improve the accuracy of the learned classification model. We perform a theoretical investigation of the problem and present three criteria (i.e., minimum redundancy, maximum uncertainty and maximum impact) to quantify the informativeness of a set of selected instances. We define an objective function based on the three criteria and present an efficient algorithm to optimize the objective function with a bounded approximation rate. Experimental results on a real-world data sets demonstrate the effectiveness of our proposed approach.
Influence is a complex and subtle force that governs the dynamics of social networks as well as the behaviors of involved users. Understanding influence can benefit various applications such as viral marketing, recommendation, and information retrieval. However, most existing works on social influence analysis have focused on verifying the existence of social influence. Few works systematically investigate how to mine the strength of direct and indirect influence between nodes in heterogeneous networks. To address the problem, we propose a generative graphical model which utilizes the heterogeneous link information and the textual content associated with each node in the network to mine topic-level direct influence. Based on the learned direct influence, a topic-level influence propagation and aggregation algorithm is proposed to derive the indirect influence between nodes. We further study how the discovered topic-level influence can help the prediction of user behaviors. We validate the approach on three different genres of data sets: Twitter, Digg, and citation networks. Qualitatively, our approach can discover interesting influence patterns in heterogeneous networks. Quantitatively, the learned topic-level influence can greatly improve the accuracy of user behavior prediction.
Recent advances in satellite tracking technologies can provide huge amount of data for biologists to understand continuous long movement patterns of wild bird species. In particular, highly correlated habitat areas are of great biological interests. Biologists can use this information to strive potential ways for controlling highly pathogenic avian influenza. We convert these biological problems into graph mining problems. Traditional models for frequent graph mining assign each vertex label with equal weight. However, the weight difference between vertexes can make strong impact on decision making by biologists. In this paper, by considering different weights of individual vertex in the graph, we develop a new algorithm, Helen, which focuses on identifying cliques with high weights. We introduce “graph-weighted support framework” to reduce clique candidates, and then filter out the low weighted cliques. We evaluate our algorithm on real life birds’ migration data sets, and show that graph mining can be very helpful for ecologists to discover unanticipated bird migration relationships.
This paper proposes a driver fatigue eye features detection algorithm based on OpenCV image processing and computer vision development platform. This algorithm localizes eye-area and detects its state based on rough to accurate thought, and can localize eye pupils in eye-open state accurately, which has significance to decrease traffic accidents. The experiment shows this algorithm can detect drivers’ eye states accurately in real-time and be of robustness and effectiveness.
Iterative message passing algorithms (iMPAs) which are generalized from the well-known turbo principle can reach a rapid pseudo-noise (PN) sequence acquisition at low computational complexity. However, its performance will degrade at low signal-to-noise ratio (SNR). In this paper, a soft information improvement using multiple samples in one chip is proposed. Meanwhile, to mitigate the timing error which will affect the information improvement, a Maximum-Likelihood (ML) estimation without significant increase on the complexity is introduced. Simulation results show that proposed method can realize rapid PN code acquisition at lower SNR than existing method.
Expertise matching, aiming to find the alignment between experts and queries, is a common problem in many real applications such as conference paper-reviewer assignment, product-reviewer alignment, and product-endorser matching. Most of existing methods for this problem usually find “relevant” experts for each query independently by using, e.g., an information retrieval method. However, in real-world systems, various domain-specific constraints must be considered. For example, to review a paper, it is desirable that there is at least one senior reviewer to guide the reviewing process. An important question is: “Can we design a framework to efficiently find the optimal solution for expertise matching under various constraints?” This paper explores such an approach by formulating the expertise matching problem in a constraint based optimization framework. Interestingly, the problem can be linked to a convex cost flow problem, which guarantees an optimal solution under given constraints. We also present an online matching algorithm to support incorporating user feedbacks in real time. The proposed approach has been evaluated on two different genres of expertise matching problems. Experimental results validate the effectiveness of the proposed approach.
We study the multicast capacity of large-scale random extended multihop wireless networks, where a number of wireless nodes are randomly located in a square region with side length a = √n, by use of Poisson distribution with density 1. All nodes transmit at a constant power P, and the power decays with attenuation exponent α 2. The data rate of a transmission is determined by the SINR as B log(1 + SINR), where B is the bandwidth. There are ns randomly and independently chosen multicast sessions. Each multicast session has k randomly chosen terminals. We show that when k ≤ θ1 n/(log n)2α+6 and ns ≥ θ2n1/2+β, the capacity that each multicast session can achieve, with high probability, is at least c8 √n/ns√k, where θ1, θ2, and c8 are some special constants and β O is any positive real number. We also show that for k = O(n/log2n), the per-flow multicast capacity under Gaussian channel is at most O(√n/ns√k) when we have at least ns = Ω(log n) random multicast flows. Our result generalizes the unicast capacity for random networks using percolation theory.
In this article, we study the set cover games when the elements are selfish agents, each of which has a privately known valuation of receiving the service from the sets, i.e., being covered by some set. Each set is assumed to have a fixed cost. We develop several approximately efficient strategyproof mechanisms that decide, after soliciting the declared bids by all elements, which elements will be covered, which sets will provide the coverage to these selected elements, and how much each element will be charged. For single-cover set cover games, we present a mechanism that is at least 1d"m"a"x-efficient, i.e., the total valuation of all selected elements is at least 1d"m"a"x fraction of the total valuation produced by any mechanism. Here, d"m"a"x is the maximum size of the sets. For multi-cover set cover games, we present a budget-balanced strategyproof mechanism that is 1d"m"a"xH"d"""m"""a"""x-efficient under reasonable assumptions. Here, H"n is the harmonic function. For the set cover games when both sets and elements are selfish agents, we show that a cross-monotonic payment-sharing scheme does not necessarily induce a strategyproof mechanism.
We study the problem of \emph{gateway placement for cost minimization} (GPCM) in two-dimensional wireless mesh networks. We are given a set of mesh routers, assume they have identical transmission range $r$, represented by unit transmission disks around them. A router may be selected as a gateway at certain \emph{placing} cost. A router is served by a gateway if and only if the gateway is within its transmission range. The goal of this work is to select a set of mesh routers as gateways to serve the rest routers with minimum overall cost. This problem is NP-hard. To the best of our knowledge, no distributed algorithm with a constant approximation ratio has been given before. When all weights are uniform, the best approximation ratio is $38$. We present both centralized and distributed algorithms which can achieve approximation ratios $6+\epsilon$ and $20$ respectively. Our algorithms greatly improve the best approximation ratios.
Community Question Answering (CQA) services have evolved into a popular way of information seeking and providing. User-posted questions in CQA are generally organized into hierarchical categories. In this paper, we define and study a novel problem which is referred to as New Category Identification (NCI) in CQA question archives. New Category Identification is primarily concerned with detecting and characterizing new or emerging categories which are not included in the existing category hierarchy. We define this problem formally, and propose both unsupervised and semi-supervised topic modeling methods to solve it. Experiments with a ground-truth set built from Yahoo! Answers show that our methods identify and interpret new categories effectively.
In this paper, we define and study a novel problem which is referred to as Community Question Grouping (CQG). Online QA services such as Yahoo! Answers contain large archives of community questions which are posted by users. Community Question Grouping is primarily concerned with grouping a collection of community questions into predefined categories. We first investigate the effectiveness of two basic methods, i.e., K-means and PLSA, in solving this problem. Then, both methods are extended in different ways to include user information. The experimental results with real datasets show that incorporation of user information improves the basic methods significantly. In addition, performance comparison reveals that PLSA with regularization is the most effective solution to the CQG problem.
Collaborative filtering (CF) is an important and popular technology for recommendation systems. However, current collaborative filtering methods suffer from some problems such as sparsity problem, inaccurate recommendation and producing big-error predictions. In this paper, we borrow ideas of object typicality from cognitive psychology and propose a novel typicality-based collaborative filtering recommendation method named TyCo. A distinct feature of typicality-based CF is that it finds ‘neighbors’ of users based on user typicality degrees in user groups (instead of the co-rated items of users or common users of items in traditional CF). To the best of our knowledge, there is no work on investigating collaborative filtering recommendation by combining object typicality. We conduct experiments to validate TyCo and compare it with previous methods.
Current recommendation methods are mainly classified into content-based, collaborative filtering and hybrid methods. These methods are based on similarity measurements among items or users. In this paper, we investigate recommendation systems from a new perspective based on object typicality and propose a novel typicality-based recommendation approach. Experiments show that our method outperforms compared methods on recommendation quality.
As the rapid development of all kinds of online databases, huge heterogeneous information networks thus derived are ubiquitous. Detecting evolutionary communities in these networks can help people better understand the structural evolution of the networks. However, most of the current community evolution analysis is based on the homogeneous networks, while a real community usually involves different types of objects in a heterogeneous network. For example, when referring to a research community, it contains a set of authors, a set of conferences or journals and a set of terms. In this paper, we study the problem of detecting evolutionary multi-typed communities defined as net-clusters in dynamic heterogeneous networks. A Dirichlet Process Mixture Model-based generative model is proposed to model the community generations. At each time stamp, a clustering of communities with the best cluster number that can best explain the current and historical networks are automatically detected. A Gibbs sampling-based inference algorithm is provided to inference the model. Also, the evolution structure can be read from the model, which can help users better understand the birth, split and death of communities. Experiments on two real datasets, namely DBLP and Delicious.com, have shown the effectiveness of the algorithm.
In traditional Gene Expression Programming (GEP), each chromosome is expressed and evaluated on the Expression Tree (ET). The ET-based expression and evaluation are computationally expensive and the intelligibility of the chromosome is low. In this paper, a highly efficient algorithm, Reduced-GEP, is proposed to solve these problems. First, the chromosome is reduced by Reduced-GEP. Second, chromosomes are evaluated directly on the reduced gene without being expressed them into ETs. In this way, the efficiency of the fitness evaluation is greatly improved. Moreover, the result of the evolution by Reduced-GEP is simplified and easier to be understood and explained. Extensive experiments demonstrate that Reduced-GEP algorithm is effective to calculate the fitness and reduce the chromosome.
Human emotion is one important underlying force affecting and affected by the dynamics of social networks. An interesting question is “can we predict a person’s mood based on his historic emotion log and his social network?”. In this paper, we propose a Mood Cast method based on a dynamic continuous factor graph model for modeling and predicting users’ emotions in a social network. Mood Cast incorporates users’ dynamic status information (e.g., locations, activities, and attributes) and social influence from users’ friends into a unified model. Based on the historical information (e.g., network structure and users’ status from time 0 to t−1), Mood Cast learns a discriminative model for predicting users’ emotion status at time t. To the best of our knowledge, this work takes the first step in designing a principled model for emotion prediction in social networks. Our experimental results on both real social network and virtual web-based network show that we can accurately predict emotion status of more than 62% of users and 8+% improvement than the baseline methods.
Mobile is becoming a ubiquitous platform for context-aware intelligent computing. One fundamental but usually ignored issue is how to efficiently manage (e.g., index and query) the mobile context data. To this end, we present a unified framework and have developed a toolkit, referred to as MQuery. More specifically, the mobile context data is represented in the standard RDF (Resource Description Framework) format. We propose a compressed-index method which takes less than 50% of the memory cost (of the traditional method) to index the context data. Four query interfaces have been developed for efficiently querying the context data including: instance query, neighbor query, shortest path query, and connection subgraph query. Experimental results on two real datasets demonstrate the efficiency of MQuery.
Retweeting is an important action (behavior) on Twitter, indicating the behavior that users re-post microblogs of their friends. While much work has been conducted for mining textual content that users generate or analyzing the social network structure, few publications systematically study the underlying mechanism of the retweeting behaviors. In this paper, we perform an interesting analysis for the problem on Twitter. We have found that almost 25.5% of the tweets posted by users are actually retweeted from friends' blog spaces. Our investigation unveils that for the retweet behaviors, some statistics still follows the power law distribution, while some others violate the state-of-the-art distribution for Web. Based on these important observations, we propose a factor graph model to predict users' retweeting behaviors. Experimental results on the Twitter data set show that our method can achieve a precision of 28.81% and recall of 37.33% for prediction of the retweet behaviors.
In this paper, we consider a novel problem referred to as term filtering with bounded error to reduce the term (feature) space by eliminating terms without (or with bounded) information loss. Different from existing works, the obtained term space provides a complete view of the original term space. More interestingly, several important questions can be answered such as: 1) how different terms interact with each other and 2) how the filtered terms can be represented by the other terms. We perform a theoretical investigation of the term filtering problem and link it to the Geometric Covering By Discs problem, and prove its NP-hardness. We present two novel approaches for both loss less and lossy term filtering with bounds on the introduced error. Experimental results on multiple text mining tasks validate the effectiveness of the proposed approaches.
Sponsored advertisement(ad) has already become the major source of revenue for most popular search engines. One fundamental challenge facing all search engines is how to achieve a balance between the number of displayed ads and the potential annoyance to the users. Displaying more ads would improve the chance for the user clicking an ad. However, when the ads are not really relevant to the users' interests, displaying more may annoy them and even "train" them to ignore ads. In this paper, we study an interesting problem that how many ads should be displayed for a given query. We use statistics on real ads click-through data to show the existence of the problem and the possibility to predict the ideal number. There are two main observations: 1) when the click entropy of a query exceeds a threshold, the CTR of that query will be very near zero; 2) the threshold of click entropy can be automatically determined when the number of removed ads is given. Further, we propose a learning approach to rank the ads and to predict the number of displayed ads for a given query. The experimental results on a commercial search engine dataset validate the effectiveness of the proposed approach.
We show that information about social relationships can be used to improve user-level sentiment analysis. The main motivation behind our approach is that users that are somehow "connected" may be more likely to hold similar opinions; therefore, relationship information can complement what we can extract about a user's viewpoints from their utterances. Employing Twitter as a source for our experimental data, and working within a semi-supervised framework, we propose models that are induced either from the Twitter follower/followee network or from the network in Twitter formed by users referring to each other using "@" mentions. Our transductive learning results reveal that incorporating social-network information can indeed lead to statistically significant sentiment classification improvements over the performance of an approach based on Support Vector Machines having access only to textual features.
Social influence, the phenomenon that the actions of a user can induce her/his friends to behave in a similar way, plays a key role in many (online) social systems. For example, a company wants to market a new product through the effect of ``word of mouth'' in the social network. It wishes to find and convince a small number of influential users to adopt the product, and the goal is to trigger a large cascade of further adoptions. Fundamentally, we need to answer the following question: how to quantify the influence between two users in a large social network? To address this question, we propose a pair wise factor graph (PFG) model to model the social influence in social networks. An efficient algorithm is designed to learn the model and make inference. We further propose a dynamic factor graph (DFG) model to incorporate the time information. Experimental results on three different genres of data sets show that the proposed approaches can efficiently infer the dynamic social influence. The results are applied to the influence maximization problem, which aims to find a small subset of nodes (users) in a social network that could maximize the spread of influence. Experiments show that the proposed approach can facilitate the application.
The presence of social networks in complex systems has made networks and community structure a focal point of study in many domains. Previous studies have focused on the structural emergence and growth of communities and on the topics displayed within the network. However, few scholars have closely examined the relationship between the thematic and structural properties of networks. Therefore, this article proposes the Tagger Tag Resource-Latent Dirichlet Allocation-Community model (TTR-LDA-Community model), which combines the Latent Dirichlet Allocation (LDA) model with the Girvan-Newman community detection algorithm through an inference mechanism. Using social tagging data from Delicious, this article demonstrates the clustering of active taggers into communities, the topic distributions within communities, and the ranking of taggers, tags, and resources within these communities. The data analysis evaluates patterns in community structure and topical affiliations diachronically. The article evaluates the effectiveness of community detection and the inference mechanism embedded in the model and finds that the TTR-LDA-Community model outperforms other traditional models in tag prediction. This has implications for scholars in domains interested in community detection, profiling, and recommender systems. © 2011 Wiley Periodicals, Inc.
Barrier coverage is an important problem for sensor networks to fulfill some given sensing tasks. Barrier coverage guarantees the detection of events happened crossing a barrier of sensors. In majority study of barrier coverage using sensor networks, sensors are assumed to have an isotropic sensing model. However, in many applications such as monitoring an area using video camera, the sensors have directional sensing model. In this paper, we investigate strong barrier coverage using directional sensors, where sensors have arbitrarily tunable orientations to provide good coverage. We investigate the problem of finding appropriate orientations of directional sensors such that they can provide strong barrier coverage. By exploiting geographical relations among directional sensors and deployment region boundaries, we first introduce the concept of virtual node to reduce the solution space from continuous domain to discrete domain. We then construct a directional barrier graph (DBG) to model this barrier coverage question such that we can quickly answer whether there are directional sensors' orientations that can provide strong barrier coverage over a given belt region. If the belt region is strong barrier covered, we then develop energy-efficient solutions to find strong barrier path(s) that will approximately minimize the total or the maximum rotation angles of all directional sensors. Extensive simulations are conducted to verify the effectiveness of our solution.
In this paper, we present a topic level expertise search framework for heterogeneous networks. Different from the traditional Web search engines that perform retrieval and ranking at document level (or at object level), we investigate the problem of expertise search at topic level over heterogeneous networks. In particular, we study this problem in an academic search and mining system, which extracts and integrates the academic data from the distributed Web. We present a unified topic model to simultaneously model topical aspects of different objects in the academic network. Based on the learned topic models, we investigate the expertise search problem from three dimensions: ranking, citation tracing analysis, and topical graph search. Specifically, we propose a topic level random walk method for ranking the different objects. In citation tracing analysis, we aim to uncover how a piece of work influences its follow-up work. Finally, we have developed a topical graph search function, based on the topic modeling and citation tracing analysis. Experimental results show that various expertise search and mining tasks can indeed benefit from the proposed topic level analysis approach.
Singular value decomposition (SVD) method is a very important matrix decomposition method in linear algebra. It is widely used in signal processing, statistics, data compression and other fields. The paper introduces a SVD method to reduce dimension of original dataset and makes use of the attribute of LSA technique to combine SVD method with LSA technique, and then presents new methods for dual private protection data mining. Finally we conduct experiments to test and verify the proposed approach and get good results.
When searching for entities with a strong local character (e.g., a museum), people may also be interested in discovering proximal activity-related entities (e.g., a café). Geographical proximity is a necessary, but not sufficient, qualifier for recommending other entities such that they are related in a useful manner (e.g., interest in a fish market does not imply interest in nearby bookshops, but interest in other produce stores is more likely). We describe and evaluate methods to identify such activity-related local entities.
We study the problem of topic-level social network search, which aims to find who are the most influential users in a network on a specific topic and how the influential users connect with each other. We employ a topic model to find topical aspects of each user and a retrieval method to identify influential users by combining the language model and the topic model. An influence maximization algorithm is then presented to find the sub network that closely connects the influential users. Two demonstration systems have been developed and are online available. Empirical analysis based on the user's viewing time and the number of clicks validates the proposed methodologies.
As Extensible Markup Language (XML) becomes prevalent in cloud computing environments, it also introduces significant performance overheads. In this paper, we analyze the performance of XML parsing, identify that a significant fraction of the performance overhead is indeed incurred by memory data loading. To address this problem, we propose implementing memory-side acceleration on top of computation-side acceleration of XML parsing. To this end, we study the impact of memory-side acceleration on performance, and evaluate its implementation feasibility including bus bandwidth utilization, hardware cost, and energy consumption. Our results show that this technique is able to improve performance by up to 20% as well as produce up to 12.77% of energy saving when implemented in 32 nm technology.
Data prefetching has been a successful technique in high-performance computing platforms. However, the conventional wisdom is that they significantly increase energy consumption, and thus not suitable for embedded mobile systems. On the other hand, as modern mobile applications pose an increasing demand for high performance, it becomes essential to implement high-performance techniques, such as prefetching, in these systems. In this paper, we study the impact of prefetching on the performance and energy consumption of embedded mobile systems. Contrary to the conventional wisdom, our findings demonstrate that as technology advances, prefetching can be energy-efficient while improving performance. Furthermore, we have developed a simple but effective analytical model to help system designers to identify the conditions for energy efficiency.
Nonnegative matrix factorization method is a kind of new matrix decomposition method. It is an effective tool for large data processing and analysis. At the same time, NMF has an important performance on intelligent information processing and pattern recognition. This paper first analyses and discusses the NMF algorithms based on its basic theory. We then propose new methods of data clustering and classification based on NMF separately. NMF method is applied to reduce the dimension of the original matrix. We run clustering algorithms on the encoded matrix after NMF processing instead of on the original matrix. Running clustering algorithms on smaller encoded matrix can save more time and storage space. After that, we bring in a series of improvement methods of classification on the basis of clustering. Finally we have done experiments to test and verify them, and gotten good results.
We study the extent to which the formation of a two-way relationship can be predicted in a dynamic social network. A two-way (called reciprocal) relationship, usually developed from a one-way (parasocial) relationship, represents a more trustful relationship between people. Understanding the formation of two-way relationships can provide us insights into the micro-level dynamics of the social network, such as what is the underlying community structure and how users influence each other. Employing Twitter as a source for our experimental data, we propose a learning framework to formulate the problem of reciprocal relationship prediction into a graphical model. The framework incorporates social theories into a machine learning model. We demonstrate that it is possible to accurately infer 90% of reciprocal relationships in a dynamic network. Our study provides strong evidence of the existence of the structural balance among reciprocal relationships. In addition, we have some interesting findings, e.g., the likelihood of two "elite" users creating a reciprocal relationships is nearly 8 times higher than the likelihood of two ordinary users. More importantly, our findings have potential implications such as how social structures can be inferred from individuals' behaviors.
Social influence is a complex and subtle force that governs the dynamics of social networks. In the past years, a lot of research work has been conducted to understand the spread patterns of social influence. However, most of approaches assume that influence exists between users with active social interactions, but ignore the question of what kind of influence happens between them. As such one interesting and also fundamental question is raised here: "in a social network, could the social connection reflect users'influence from both positive and negative aspects?". To this end, an Opinion Oriented Link Analysis Model (OOLAM) is proposed in this paper to characterize users' influence personae in order to exhibit their distinguishing influence ability in the social network. In particular, three types of influence personae are generalized and the problem of influence persona discovery is formally defined. Within the OOLAM model, two factors, i.e., opinion consistency and opinion creditability, are defined to capture the persona information from public opinion perspective. Extensive experimental studies have been performed to demonstrate the effectiveness of the proposed approach on influence persona analysis using real web data sets.
In many social networks, there exist two types of users that exhibit different influence and different behavior. For instance, statistics have shown that less than 1% of the Twitter users (e.g. entertainers, politicians, writers) produce 50% of its content, while the others (e.g. fans, followers, readers) have much less influence and completely different social behavior. In this paper, we define and explore a novel problem called community kernel detection in order to uncover the hidden community structure in large social networks. We discover that influential users pay closer attention to those who are more similar to them, which leads to a natural partition into different community kernels. We propose Greedy and We BA, two efficient algorithms for finding community kernels in large social networks. Greedy is based on maximum cardinality search, while We BA formalizes the problem in an optimization framework. We conduct experiments on three large social networks: Twitter, Wikipedia, and Coauthor, which show that We BA achieves an average 15%-50% performance improvement over the other state-of-the-art algorithms, and We BA is on average 6-2,000 times faster in detecting community kernels.
The validity of the web content which is craweled based on theme search does not make us very satisfied. This article adopts genetic algorithm to realize subject extraction, block processing of the page content, the validity verification with subject. Finally we grab pages that have high correlation with themes. Experimental results show that the method is feasible and improve the effectiveness of the theme search.
In most of the cases, scientists depend on previous literature which is relevant to their research fields for developing new ideas. However, it is not wise, nor possible, to track all existed publications because the volume of literature collection grows extremely fast. Therefore, researchers generally follow, or cite merely a small proportion of publications which they are interested in. For such a large collection, it is rather interesting to forecast which kind of literature is more likely to attract scientists' response. In this paper, we use the citations as a measurement for the popularity among researchers and study the interesting problem of Citation Count Prediction (CCP) to examine the characteristics for popularity. Estimation of possible popularity is of great significance and is quite challenging. We have utilized several features of fundamental characteristics for those papers that are highly cited and have predicted the popularity degree of each literature in the future. We have implemented a system which takes a series of features of a particular publication as input and produces as output the estimated citation counts of that article after a given time period. We consider several regression models to formulate the learning process and evaluate their performance based on the coefficient of determination (R-square). Experimental results on a real-large data set show that the best predictive model achieves a mean average predictive performance of 0.740 measured in R-square, which significantly outperforms several alternative algorithms.
We study the throughput capacity and transport capacity for both random and arbitrary wireless networks under Gaussian Channel model when all wireless nodes have the same constant transmission power P and the transmission rate is determined by Signal to Interference plus Noise Ratio (SINR). We consider networks with n wireless nodes $$\{v_1,v_2,\ldots,v_n\}$$ (randomly or arbitrarily) distributed in a square region B a with a side-length a. We randomly choose n s node as the source nodes of n s multicast sessions. For each source node v i , we randomly select k points and the closest k nodes to these points as destination nodes of this multicast session. We derive achievable lower bounds and some upper bounds on both throughput capacity and transport capacity for both unicast sessions and multicast sessions. We found that the asymptotic capacity depends on the size a of the deployment region, and it often has three regimes.
In this paper, we focus on designing efficient query of top-k data produced by sensor nodes in a wireless sensor network (WSN). Assume that we are given a connected WSN of diameter D, consisting of n nodes with maximum node degree ?. Two different models are studied. In the first model, each node holds a numeric element, the goal is to determine the top-k smallest (or biggest) of these elements from all nodes. In the second model, there are m objects in set L, each node v
In this paper, we study the dynamic node activation schedule for the utility based coverage problem in solar-powered wireless sensor networks. We assume that the utility achieved by a WSN for coverage service is a sub modular function over the set of sensors that will provide the service. We first present an integer programming formulation with sub modular objective functions. We then present an efficient simple greedy hill-climbing algorithm such that the achieved average utility of the computed schedule is at least $1/2$ times that achieved by the optimal schedule. To the best of our knowledge, this is the first polynomial time algorithm that can ensure a good constant approximation of the achieved utility for multi-target coverage problem. We conduct extensive evaluations to study the performances of our proposed aggregation scheduling algorithm on real testbed. Our evaluation results corroborate our theoretical analysis.
Coverage quality is one critical metric to evaluate the Quality of Service (QoS) provided by wireless sensor networks. In this paper, we address maximum support coverage problem (a.k.a. best case coverage) in wireless sensor networks. Most of the existing work assume that the coverage degree is 1, i. e. every point on the resultant path should fall within the sensing range of at least one sensor node. Here we study the k-coverage problem, in which every point on the resultant path is covered by at least k sensors while optimizing certain objectives. We present tackle this problem under both centralized and distributed setting. The time complexity is bounded by O(k2n log n) where n is the number of deployed sensor nodes. To the best of our knowledge, this is the first work that presents polynomial time algorithms that find optimal k-support paths for a general k.
Java Virtual Machine (JVM) education has become essential in training embedded software engineers as well as virtual machine researchers and practitioners. However, due to the lack of suitable instructional tools, it is difficult for students to obtain any kind of hands-on experience and to attain any deep understanding of JVM design. To address this issue, the authors designed the RHE, or Reduced Harmony for Education, a lightweight instructional JVM. The RHE is extremely simple and yet contains all essential modules of a JVM. Furthermore, it comes with several test programs designed to familiarize users with the modules of JVM. In the past few years, the authors have successfully used the RHE to train new engineers on the JVM technology. The training experience shows that with the RHE, it takes less than 40 h for engineers with little or no prior knowledge of JVM design to become familiar with the essential JVM components.
In wireless sensor and actor networks (WSANs), a set of static sensor nodes and a set of (mobile) actor nodes form a network that performs distributed sensing and actuation tasks. In [1], Abbasi et al. presented DARA, a Distributed Actor Recovery Algorithm, which restores the connectivity of the interactor network by efficiently relocating some mobile actors when failure of an actor happens. To restore 1 and 2-connectivity of the network, two algorithms are developed in [1]. Their basic idea is to find the smallest set of actors that needs to be repositioned to restore the required level of connectivity, with the objective to minimize the movement overhead of relocation. Here, we show that the algorithms proposed in [1] will not work smoothly in all scenarios as claimed and give counterexamples for some algorithms and theorems proposed in [1]. We then present a general actor relocation problem and propose methods that will work correctly for several subsets of the problems. Specifically, our method does result in an optimum movement strategy with minimum movement overhead for the problems studied in [1].
In online social networks, most relationships are lack of meaning labels (e.g., "colleague" and "intimate friends"), simply because users do not take the time to label them. An interesting question is: can we automatically infer the type of social relationships in a large network? what are the fundamental factors that imply the type of social relationships? In this work, we formalize the problem of social relationship learning into a semi-supervised framework, and propose a Partially-labeled Pairwise Factor Graph Model (PLP-FGM) for learning to infer the type of social ties. We tested the model on three different genres of data sets: Publication, Email and Mobile. Experimental results demonstrate that the proposed PLP-FGM model can accurately infer 92.7% of advisoradvisee relationships from the coauthor network (Publication), 88.0% of manager-subordinate relationships from the email network (Email), and 83.1% of the friendships from the mobile network (Mobile). Finally, we develop a distributed learning algorithm to scale up the model to real large networks.
This paper explores bridging the content of two different languages via latent topics. Specifically, we propose a unified probabilistic model to simultaneously model latent topics from bilingual corpora that discuss comparable content and use the topics as features in a cross-lingual, dictionary-less text categorization task. Experimental results on multilingual Wikipedia data show that the proposed topic model effectively discovers the topic information from the bilingual corpora, and the learned topics successfully transfer classification knowledge to other languages, for which no labeled training data are available.
Name ambiguity has long been viewed as a challenging problem in many applications, such as scientific literature management, people search, and social network analysis. When we search a person name in these systems, many documents (e.g., papers, web pages) containing that person's name may be returned. It is hard to determine which documents are about the person we care about. Although much research has been conducted, the problem remains largely unsolved, especially with the rapid growth of the people information available on the Web. In this paper, we try to study this problem from a new perspective and propose an ADANA method for disambiguating person names via active user interactions. In ADANA, we first introduce a pairwise factor graph (PFG) model for person name disambiguation. The model is flexible and can be easily extended by incorporating various features. Based on the PFG model, we propose an active name disambiguation algorithm, aiming to improve the disambiguation performance by maximizing the utility of the user's correction. Experimental results on three different genres of data sets show that with only a few user corrections, the error rate of name disambiguation can be reduced to 3.1%. A real system has been developed based on the proposed method and is available online.
Distribution static synchronous compensator can bu used to improve the voltage quaLity through exchanging the reactive power between the D-STATCOM and the power system. Keep the voltage of the point of common coulpLing constant and balance is a very important power quaLity issues. The voltage controll strategy which has the function of voltage compensation and balance of the D-STATCOM is investigated in detail in this paper. The power exchange in D-STATCOM system is analysed based on intantaneous power theory. The relationship between the positive and negtive sequence of the output voltage of D-STATCOM is derived. Then the voltage control strategy witHout current regulator is prsented which has the merits of simply structure and easy to design. The voltage controller of D-STATCOM consists of positive and negtive sequence voltage controller. which can regulate and balance the voltage simultaneously. Theory analysis and digital sinulation results verified the feasibiLity and vaLidity of the voltage control strategy proposed in this paper.
Formalizing social networks in a factor graph model and using a learning algorithm to estimate the pairwise social influence between nodes can help inform multilevel social community analysis.
We study a novel problem of social context summarization for Web documents. Traditional summarization research has focused on extracting informative sentences from standard documents. With the rapid growth of online social networks, abundant user generated content (e.g., comments) associated with the standard documents is available. Which parts in a document are social users really caring about? How can we generate summaries for standard documents by considering both the informativeness of sentences and interests of social users? This paper explores such an approach by modeling Web documents and social contexts into a unified framework. We propose a dual wing factor graph (DWFG) model, which utilizes the mutual reinforcement between Web documents and their associated social contexts to generate summaries. An efficient algorithm is designed to learn the proposed factor graph model.Experimental results on a Twitter data set validate the effectiveness of the proposed model. By leveraging the social context information, our approach obtains significant improvement (averagely +5.0%-17.3%) over several alternative methods (CRF, SVM, LR, PR, and DocLead) on the performance of summarization.
The relationships between consumer emotions and their buying behaviors have been well documented. Technology-savvy consumers often use the web to find information on products and services before they commit to buying. We propose a semantic web usage mining approach for discovering periodic web access patterns from annotated web usage logs which incorporates information on consumer emotions and behaviors through self-reporting and behavioral tracking. We use fuzzy logic to represent real-life temporal concepts (e.g., morning) and requested resource attributes (ontological domain concepts for the requested URLs) of periodic pattern-based web access activities. These fuzzy temporal and resource representations, which contain both behavioral and emotional cues, are incorporated into a Personal Web Usage Lattice that models the user's web access activities. From this, we generate a Personal Web Usage Ontology written in OWL, which enables semantic web applications such as personalized web resources recommendation. Finally, we demonstrate the effectiveness of our approach by presenting experimental results in the context of personalized web resources recommendation with varying degrees of emotional influence. Emotional influence has been found to contribute positively to adaptation in personalized recommendation.
Due to advances in low power micro-sensor technology, energy harvesting techniques, we can now build large scale solar-powered sensor networks to support long-running operations. Solar powered sensors often harvest variable amounts of energy in different weather conditions. Then a primary requirement for an efficient and a long-running solar-powered sensor system is to adapt to changing environment conditions and resources, and to gather as much valuable data as possible. Sensing and collecting data at a constant rate, without taking into account energy availability or data deliverability, will either drain the battery or waste resources. In this work, we design and test a highly efficient and robust solar-powered system SmartMote; and we further present an energy and value of information (VoI) aware routing strategy, that balances the rates of sensing with packet delivery for SmartMote. SmartMote achieves fairness and near maximum utility across the network. We deploy SmartMote in a forest with 100 sensors in order to monitor the humidity, temperature and luminance intensity. Our experimental results corroborate our design.
This paper proposes a Topic-Level Opinion Influence Model (TOIM) that simultaneously incorporates topic factor, user opinions and social influence in a unified probabilistic model with two stages learning processes. In the first stage, topic factor and user influence are integrated to generate users' influential relationship based on different topics; in the second stage, users' historical messages and social interaction records are leveraged by TOIM to construct their historical opinions and neighbors' opinion influence through a statistical learning process, which can be further utilized to predict users' future opinions on some specific topics. We evaluate our TOIM on a large-scaled dataset from Tencent Weibo, one of the largest microbloggings website in China. The experimental results show that TOIM can better predict users' opinion than other baseline methods.
The objective of this work is to evaluate the viability of implementing workload-aware dynamic power management schemes on a many-core platform, aiming at reducing power consumption for high performance computing (HPC) application. Two approaches were proposed to achieve the desired target. First approach is an off-line scheduling scheme where core voltage and frequency are set up beforehand based on the workload characterization of the application. The second approach is an on-line scheduling scheme, where core voltage and frequency are controlled based on a workload detection algorithm. Experiments were conducted using the 48-core Intel Single-chip Cloud Computer (SCC), running a parallelized Fire Spread Monte Carlo Simulation program. Both schemes were compared against a performance-driven, but non-power-aware management scheme. The results indicate that our schemes are able to reduce the power consumption up to 29\% with mild impact on the system performance.
Expert finding is concerned about finding persons who are knowledgeable on a given topic. It has many applications in enterprise search, social networks, and collaborative management. In this paper, we study the problem of diversification for expert finding. Specifically, employing an academic social network as the basis for our experiments, we aim to answer the following question: Given a query and an academic social network, how to diversify the ranking list, so that it captures the whole spectrum of relevant authors' expertise? We precisely define the problem and propose a new objective function by incorporating topic-based diversity into the relevance ranking measurement. A learning-based model is presented to solve the objective function. Our empirical study in a real system validates the effectiveness of the proposed method, which can achieve significant improvements (+15.3%-+94.6% by MAP) over alternative methods.
We study users' behavioral patterns in ephemeral social networks, which are temporarily built based on events such as conferences. From the data distribution and social theory perspectives, we found several interesting patterns. For example, the duration of two random persons staying at the same place and at the same time obeys a two-stage power-law distribution. We develop a framework to infer the likelihood of two users to meet together, and we apply the framework to two mobile social networks: UbiComp and Reality. The former is formed by researchers attending UbiComp 2011 and the latter is a network of students published by MIT. On both networks, we validate the proposed predictive framework, which significantly improve the accuracy for predicting geographic coincidence by comparing with two baseline methods.
We study the extent to which social ties between people can be inferred in large social network, in particular via active user interactions. In most online social networks, relationships are lack of meaning labels (e.g., "colleague" and "intimate friends") due to various reasons. Understanding the formation of different types of social relationships can provide us insights into the micro-level dynamics of the social network. In this work, we precisely define the problem of inferring social ties and propose a Partially-Labeled Pairwise Factor Graph Model (PLP-FGM) for learning to infer the type of social relationships. The model formalizes the problem of inferring social ties into a flexible semi-supervised framework. We test the model on three different genres of data sets and demonstrate its effectiveness. We further study how to leverage user interactions to help improve the inferring accuracy. Two active learning algorithms are proposed to actively select relationships to query users for their labels. Experimental results show that with only a few user corrections, the accuracy of inferring social ties can be significantly improved. Finally, to scale the model to handle real large networks, a distributed learning algorithm has been developed.
Citations are highly valuable for analyzing documents and have been widely studied in recent years. Among the document modeling, the citations are treated as documents' attributes just like the words in the documents; or as the degrees in graph theory. These methods add citations into word sampling process to reform the document representation but they miss the impact of the citations in the generation of content. In this paper, we view the citations as the prior information which authors have had. In the generation of document, content of the document is split into two parts: the idea of the author and the knowledge from the cited papers. We proposed a prior information enabled topic model-PLDA. In the modeling, both the document and its citations play the important role of generating the topic layer. Our experiments on two linked datasets show that our model greatly outperforms basic LDA procedures on a clustering task while also maintaining the dependencies among documents. In addition, we also show the feasibility by the task of citation recommendation.
Ride-sharing is considered as one of the promising solutions for reducing fuel consumption of fuel and reducing the congestion in urban cities, hence reducing the environmental pollution. With the advancement of mobile social networking technologies, it is necessary to reconsider the principles and desired characteristics of ride-sharing systems. Ride-sharing systems can be popular among people if we can provide more flexible and adaptive solution according to preferences of the participants and solve the social challenges. In this paper, we focus on encouraging people to use a ride-sharing system by satisfying their demands in terms of safety, privacy, convenience and also provide enough incentives for drivers and riders. We formalized the ride sharing problem as a multi source-destination path planning problem. An objective function is developed which models different conflicting objectives in a unified framework. We provide the flexibility to each driver that he can generate the sub-optimal paths according to his own requirements by suitably adjusting the weights. These sub-optimal paths are generated in an order of priority (optimality). The simulation results have shown that the system has the potential to compute multiple optimal paths.
Can we understand van Gogh's mood from his artworks? For many years, people have tried to capture van Gogh's affects from his artworks so as to understand the essential meaning behind the images and catch on why van Gogh created these works. In this paper, we study the problem of inferring affects from images in social networks. In particular, we aim to answer: What are the fundamental features that reflect the affects of the authors in images? How the social network information can be leveraged to help detect these affects? We propose a semi-supervised framework to formulate the problem into a factor graph model. Experiments on 20,000 random-download Flickr images show that our method can achieve a precision of 49% with a recall of 24% on inferring authors'affects into 16 categories. Finally, we demonstrate the effectiveness of the proposed method on automatically understanding van Gogh's Mood from his artworks, and inferring the trend of public affects around special event.
Despite years of research, the name ambiguity problem remains largely unresolved. Outstanding issues include how to capture all information for name disambiguation in a unified approach, and how to determine the number of people K in the disambiguation process. In this paper, we formalize the problem in a unified probabilistic framework, which incorporates both attributes and relationships. Specifically, we define a disambiguation objective function for the problem and propose a two-step parameter estimation algorithm. We also investigate a dynamic approach for estimating the number of people K. Experiments show that our proposed framework significantly outperforms four baseline methods of using clustering algorithms and two other previous methods. Experiments also indicate that the number K automatically found by our method is close to the actual number.
Patenting is one of the most important ways to protect company's core business concepts and proprietary technologies. Analyzing large volume of patent data can uncover the potential competitive or collaborative relations among companies in certain areas, which can provide valuable information to develop strategies for intellectual property (IP), R&D, and marketing. In this paper, we present a novel topic-driven patent analysis and mining system. Instead of merely searching over patent content, we focus on studying the heterogeneous patent network derived from the patent database, which is represented by several types of objects (companies, inventors, and technical content) jointly evolving over time. We design and implement a general topic-driven framework for analyzing and mining the heterogeneous patent network. Specifically, we propose a dynamic probabilistic model to characterize the topical evolution of these objects within the patent network. Based on this modeling framework, we derive several patent analytics tools that can be directly used for IP and R&D strategy planning, including a heterogeneous network co-ranking method, a topic-level competitor evolution analysis algorithm, and a method to summarize the search results. We evaluate the proposed methods on a real-world patent database. The experimental results show that the proposed techniques clearly outperform the corresponding baseline methods.
The impact of land use and land cover changes (LUCC) on global carbon cycle raises more and more serious concerns. Various methods were assessed to determine the influence of LUCC on soil organic carbon (SOC). However, the regional patterns and causes of terrestrial carbon sources and sinks remain uncertain. We analyze the variation of SOC stock in farming-pastoral ecotone in relation to LUCC over the past century in Zhenlai County located in the middle of northeast China. This study analyzes the characteristics of LUCC in Zhenlai County during the past century based on extensive historical literatures and remote sensing data. Based on abundant soil profile data, the SOC densities (0-30cm) in different land use and land cover types (LUCT) was analyzed. Then the change of SOC stock in the surface layer (0-30cm) caused by LUCC in this period was estimated with bookkeeping model, and we have also considered the impact of fertilizer management change. According to our results, during the past century, there was a rapid increase of cropland area from nearly zero to about 45% in the whole region, while the grassland area decreased from 68% in 1896 to 21% in 2004. Zhenlai region is a net source of CO2 to the atmosphere owing to LUCC and fertilizer managements, which caused SOC stock decreased about 48% in the past century, especially during 1980-present.
Interdisciplinary collaborations have generated huge impact to society. However, it is often hard for researchers to establish such cross-domain collaborations. What are the patterns of cross-domain collaborations? How do those collaborations form? Can we predict this type of collaborations? Cross-domain collaborations exhibit very different patterns compared to traditional collaborations in the same domain: 1) sparse connection: cross-domain collaborations are rare; 2) complementary expertise: cross-domain collaborators often have different expertise and interest; 3) topic skewness: cross-domain collaboration topics are focused on a subset of topics. All these patterns violate fundamental assumptions of traditional recommendation systems. In this paper, we analyze the cross-domain collaboration data from research publications and confirm the above patterns. We propose the Cross-domain Topic Learning (CTL) model to address these challenges. For handling sparse connections, CTL consolidates the existing cross-domain collaborations through topic layers instead of at author layers, which alleviates the sparseness issue. For handling complementary expertise, CTL models topic distributions from source and target domains separately, as well as the correlation across domains. For handling topic skewness, CTL only models relevant topics to the cross-domain collaboration. We compare CTL with several baseline approaches on large publication datasets from different domains. CTL outperforms baselines significantly on multiple recommendation metrics. Beyond accurate recommendation performance, CTL is also insensitive to parameter tuning as confirmed in the sensitivity analysis.
Although virtualization technologies bring many benefits to cloud computing environments, as the virtual machines provide more features, the middleware layer has become bloated, introducing a high overhead. Our ultimate goal is to provide hardware-assisted solutions to improve the middleware performance in cloud computing environments. As a starting point, in this paper, we design, implement, and evaluate specialized hardware instructions to accelerate GC operations. We select GC because it is a common component in virtual machine designs and it incurs high performance and energy consumption overheads. We performed a profiling study on various GC algorithms to identify the GC performance hotspots, which contribute to more than 50% of the total GC execution time. By moving these hotspot functions into hardware, we achieved an order of magnitude speedup and significant improvement on energy efficiency. In addition, the results of our performance estimation study indicate that the hardware-assisted GC instructions can reduce the GC execution time by half and lead to a 7% improvement on the overall execution time.
It is well known that different types of social ties have essentially different influence on people. However, users in online social networks rarely categorize their contacts into "family", "colleagues", or "classmates". While a bulk of research has focused on inferring particular types of relationships in a specific social network, few publications systematically study the generalization of the problem of inferring social ties over multiple heterogeneous networks. In this work, we develop a framework for classifying the type of social relationships by learning across heterogeneous networks. The framework incorporates social theories into a factor graph model, which effectively improves the accuracy of inferring the type of social relationships in a target network by borrowing knowledge from a different source network. Our empirical study on five different genres of networks validates the effectiveness of the proposed framework. For example, by leveraging information from a coauthor network with labeled advisor-advisee relationships, the proposed framework is able to obtain an F1-score of 90% (8-28% improvements over alternative methods) for inferring manager-subordinate relationships in an enterprise email network.
Marketing strategies without emotion will not work. Emotion stimulates the mind 3,000 times quicker than rational thought. Such emotion invokes either a positive or a negative response and physical expressions. Understanding the underlying dynamics of users' emotions can efficiently help companies formulate marketing strategies and support after-sale services. While prior work has focused mainly on qualitative aspects, in this paper we present our research on quantitative analysis of how an individual's emotional state can be inferred from her historic emotion log and how this person's emotional state influences (or is influenced by) her friends in the social network. We statistically study the dynamics of individual's emotions and discover several interesting as well as important patterns. Based on this discovery, we propose an approach referred to as MoodCast to learn to infer individuals' emotional states. In both mobile-based social network and online virtual network, we verify the effectiveness of our proposed approach.
We study a novel problem of batch mode active learning for networked data. In this problem, data instances are connected with links and their labels are correlated with each other, and the goal of batch mode active learning is to exploit the link-based dependencies and node-specific content information to actively select a batch of instances to query the user for learning an accurate model to label unknown instances in the network. We present three criteria (i.e., minimum redundancy, maximum uncertainty, and maximum impact) to quantify the informativeness of a set of instances, and formalize the batch mode active learning problem as selecting a set of instances by maximizing an objective function which combines both link and content information. As solving the objective function is NP-hard, we present an efficient algorithm to optimize the objective function with a bounded approximation rate. To scale to real large networks, we develop a parallel implementation of the algorithm. Experimental results on both synthetic datasets and real-world datasets demonstrate the effectiveness and efficiency of our approach.
Current user-definable websites are faced with some challenges such as the limited services, the ineffective profit models, and the lack of the users' participation in the services provision and so on. Here a user-definable website is implemented based on the technologies of web2.0 and asp.net. This system could provide three kernel functions of customizing the system services, sharing the internet services and uploading the self-developed services by the users themselves.
This article investigates the dynamic features of social tagging vocabularies in Delicious, Flickr, and YouTube from2003 to 2008. Three algorithms are designed to study the macro- and micro-tag growth as well as the dynamics of taggers' activities, respectively. Moreover, we propose a Tagger Tag Resource Latent Dirichlet Allocation (TTRLDA) model to explore the evolution of topics emerging from those social vocabularies. Our results show that (a) at the macro level, tag growth in all the three tagging systems obeys power law distribution with exponents lower than 1; at the micro level, the tag growth of popular resources in all three tagging systems follows a similar power law distribution; (b) the exponents of tag growth vary in different evolving stages of resources; (c) the growth of number of taggers associated with different popular resources presents a feature of convergence over time; (d) the active level of taggers has a positive correlation with the macro-tag growth of different tagging systems; and (e) some topics evolve into several subtopics over time while others experience relatively stable stages in which their contents do not change much, and certain groups of taggers continue their interests in them.
Usually scientists breed research ideas inspired by previous publications, but they are unlikely to follow all publications in the unbounded literature collection. The volume of literature keeps on expanding extremely fast, whilst not all papers contribute equal impact to the academic society. Being aware of potentially influential literature would put one in an advanced position in choosing important research references. Hence, estimation of potential influence is of great significance. We study a challenging problem of identifying potentially influential literature. We examine a set of hypotheses on what are the fundamental characteristics for highly cited papers and find some interesting patterns. Based on these observations, we learn to identify potentially influential literature via Future Influence Prediction (FIP), which aims to estimate the future influence of literature. The system takes a series of features of a particular publication as input and produces as output the estimated citation counts of that article after a given time period. We consider several regression models to formulate the learning process and evaluate their performance based on the coefficient of determination (R2). Experimental results on a real-large data set show a mean average predictive performance of 83.6% measured in R^2. We apply the learned model to the application of bibliography recommendation and obtain prominent performance improvement in terms of Mean Average Precision (MAP).
In this paper, we study a new problem of instant social graph search, which aims to find a sub graph that closely connects two and more persons in a social network. This is a natural requirement in our real daily life, such as "Who can be my referrals for applying for a job position?". In this paper, we formally define the problem and present a series of approximate algorithms to solve this problem: Path, Influence, and Diversity. To evaluate the social graph search results, we have developed two prototype systems, which are online available and have attracted thousands of users. In terms of both user's viewing time and the number of user clicks, we demonstrate that the three algorithms can significantly outperform (+34.56%-+131.37%) the baseline algorithm.
This paper presents the solution of the team "ISSSID" for the Consumer Products Contest #1(CPROD1) of ICDM 2012. The contest provides a dataset including hundreds of thousands of text items, a product catalog with over fifteen million products, and hundreds of manually annotated product mentions. The goal of the competition is to automatically recognize product mentions in the textual content and disambiguate which product(s) in the product catalog are referenced by the mentions. We propose a hybrid approach which combines the results obtained by several separately trained recognition models. Specifically, the approach uses a standard matching model, a rule template model, and a conditional random field model, and finally combines the results using a blending model. The proposed approach achieves the best performance in the contest.
With the recent introduction of Spot Instances in the Amazon Elastic Compute Cloud (EC2), users can bid for resources and thus control the balance of reliability versus monetary costs. Mechanisms and tools that deal with the cost-reliability trade-offs under this schema are of great value for users seeking to lessen their costs while maintaining high reliability. In this paper, we propose a set of bidding strategies to minimize the cost and volatility of resource provisioning. Essentially, to derive an optimal bidding strategy, we formulate this problem as a Constrained Markov Decision Process (CMDP). Based on this model, we are able to obtain an optimal randomized bidding strategy through linear programming. Using real Instance Price traces and workload models, we compare several adaptive check-pointing schemes in terms of monetary costs and job completion time. We evaluate our model and demonstrate how users should bid optimally on Spot Instances to reach different objectives with desired levels of confidence.
The fundamental challenge of garbage collector (GC) design is to maximize the recycled space with minimal time overhead. For efficient memory management, in many GC designs the heap is divided into large object space (LOS) and normal object space (non-LOS). When either space is full, garbage collection is triggered even though the other space may still have plenty of room, thus leading to inefficient space utilization. Also, space partitioning in existing GC designs implies different GC algorithms for different spaces. This not only prolongs the pause time of garbage collection, but also makes collection inefficient on multiple spaces. To address these problems, we propose Packer, a parallel garbage collection algorithm based on the novel concept of virtual spaces. Instead of physically dividing the heap into multiple spaces, Packer manages multiple virtual spaces in one physical space. With multiple virtual spaces, Packer offers efficient memory management. With one physical space, Packer avoids the problem of an inefficient space utilization. To reduce the garbage collection pause time, we also propose a novel parallelization method that is applicable to multiple virtual spaces. Specifically, we reduce the compacting GC parallelization problem into a discreted acyclic graph (DAG) traversal parallelization problem, and apply it to both normal and large object compaction.
As a part of trials to develop the antitumor agent from tannins, the antitumor activity of pedunculagin, an ellagitannin, isolated from eucalyptus leaves, was reported in this paper. In vitro, the antitumor activity was determined by MTT assay. Pedunculagin showed the dose-dependent antitumor activity against human liver tumor cells (QGY-7703), the 50% inhibition concentration (IC50) was (64.3卤6.1)µg/mL, Under microscope, holes-like structure was observed on the membrane of tumor cells, which co-cultured with pedunculagin, but it was negative for human normal cell (HEB), this means pedunculagin can change the structure of tumor cell membrane specifically, when 5-Fu was mixed with 5µg/mL of pedunculagin, the IC50 of 5-Fu greatly reduced to (0.9卤0.6) µg/mL from (5.9卤2.2) µg/mL, the probably reason is that pedunculagin can improve the permeability of tumor cell membrane. This paper first reports the phenomena of holes-like structure on the membrane of tumor cells caused by pedunculagin.
This paper studies the problem of expertise matching with various constraints. Expertise matching, which aims to find the alignment between experts and queries, is a common problem in many applications such as conference paper-reviewer assignment, product-reviewer alignment, and product-endorser matching. Most existing methods formalize this problem as an information-retrieval problem and focus on finding a set of experts for each query independently. However, in real-world systems, various constraints are often needed to be considered. For example, in order to review a paper, it is desirable that there is at least one senior reviewer to guide the reviewing process. An important question is: ''Can we design a framework to efficiently find the optimal solution for expertise matching under various constraints?'' This paper explores such an approach by formulating the expertise matching problem in a constraint-based optimization framework. In the proposed framework, the problem of expertise matching is linked to a convex cost flow problem, which guarantees an optimal solution under various constraints. We also present an online matching algorithm to support incorporating user feedbacks in real time. The proposed approach has been evaluated on two different genres of expertise matching problems, namely conference paper-reviewer assignment and teacher-course assignment. Experimental results validate the effectiveness of the proposed approach. Based on the proposed method, we have also developed an online system for paper-reviewer suggestions, which has been used for paper-reviewer assignment in a top conference and feedbacks from the conference organizers are very positive.
In this paper, we study periodic query scheduling for data aggregation with minimum delay under various wireless interference models. Given a set Q of periodic aggregation queries, each query Qi ε Q has its own period pi and the subset of source nodes Sicontaining the data. We first propose a family of efficient and effective real-time scheduling protocols that can answer every job of each query task Qi ε Q within a relative delay O(pi) under resource constraints by addressing the following tightly coupled tasks: routing, transmission plan constructions, node activity scheduling, and packet scheduling. Based on our protocol design, we further propose schedulability test schemes to efficiently and effectively test whether, for a set of queries, each query job can be finished within a finite delay. Our theoretical analysis shows that our methods achieve at least a constant fraction of the maximum possible total utilization for query tasks, where the constant depends on wireless interference models. We also conduct extensive simulations to validate the proposed protocol and evaluate its practical performance. The simulations corroborate our theoretical analysis.
With the blessing of information technology, we are living in an increasingly networked world. People, information and other entities are connected via World Wide Web, email networks, instant messaging networks, mobile communication networks, online social networks, etc. These online networks grow fast and possess huge amount of recorded information, which presents great opportunities in understanding the science of these networks, and in developing new applications from these networks and for these networks. The increasingly networked society has fundamentally changed our way of thinking, individual behaviors and social activities. It is foreseen that the public health relating to epidemic diseases is greatly impacted by this emerging connectivity as they are by nature mediated by direct or indirect human interactions and mobility. However, new challenges have to be met --- the networks are huge and information is noisy, and they demand new methodologies in accessing and analyzing these networks, and in developing theories and applications for the networks. To meet with these challenges, researchers from a wide range of academic fields, including theory and algorithms, data mining and machine learning, computer systems and networks, statistical physics and complex systems, sociology, social psychology, economics and managerial science, etc. are all actively studying various aspects concerning social and information networks. However, we lack the proper opportunities for people from these diverse backgrounds to directly interact with each other. The diversity of approaches and methodologies to study various social networks has raised the need for an interdisciplinary effort to create the required expertise to address the fundamental open questions in this field. This workshop is intended to present such an opportunity and serve as a forum to bring together people from various fields to exchange their latest research results and to sparkle new ideas and directions to properly understand these networks. It will be held on August 12, 2012, in conjunction with ACM KDD 2012, August 12-16, Beijing, China.
Social influence in social networks has been extensively researched. Most studies have focused on direct influence, while another interesting question can be raised as whether indirect influence exists between two users who're not directly connected in the network and what affects such influence. In addition, the theory of complex contagion tells us that more spreaders will enhance the indirect influence between two users. Our observation of intensity of indirect influence, propagated by n parallel spreaders and quantified by retweeting probability in two Twitter social networks, shows that complex contagion is validated globally but is violated locally. In other words, the retweeting probability increases non-monotonically with some local drops. A quantum cognition based probabilistic model is proposed to account for these local drops.
In this paper, we study k-road-coverage problems in wireless sensor networks (WSNs). Assume there is a 2-dimensional area Ω with a given road map **image**  = (V,E) where E contains all road segments and V consists of all intersection points on Ω. The first question we study is about ‘sensor deployment’, i.e., how to deploy a minimum number of sensor nodes on Ω such that each path (each road segment) on **image** is k-covered when all sensor nodes have the same sensing range. When sensors can only be deployed in a set of discrete locations, we propose an efficient method with the approximation ratio 6 + ϵ for the special case where k = 1 and O(k) generally. If sensors can be deployed in arbitrary locations, we propose an efficient method with the approximation ratio 24 + ϵ when k = 1 and O(k) generally. The second question we study is about ‘path query’, i.e., how to find the k-covered path or k-support path connecting any given source/destination pair of points on the road map **image**. Basically, given any source/destination pair of points S and D, we present two algorithms which can efficiently find a k-covered path connecting S and D and a k-supported path connecting S and D, respectively. Copyright © 2010 John Wiley & Sons, Ltd.
Threaded prefetching based on Chip Multiprocessor (CMP) issues memory requests for data needed later by the main computation, and therefore may lead to increased stress on limited shared cache space and bus bandwidth. In our earlier work, we had proposed an effective threaded prefetching technique that selects proper prefetch distance for specific application to improve the timeliness of prefetching. In this paper, we first estimate the upper limit of prefetch distance for specific application in our proposed threaded prefetching technique, and then analyze the effect of increasing prefetch distance on shared cache pollution. Our experimental evaluations indicated that the bounded range of effective prefetch distance can be determined using our method, and the shared cache pollution can be reduced by controlling prefetch distance in our proposed threaded prefetching technique.
Detecting and monitoring competitors is fundamental to a company to stay ahead in the global market. Existing studies mainly focus on mining competitive relationships within a single data source, while competing information is usually distributed in multiple networks. How to discover the underlying patterns and utilize the heterogeneous knowledge to avoid biased aspects in this issue is a challenging problem. In this paper, we study the problem of mining competitive relationships by learning across heterogeneous networks. We use Twitter and patent records as our data sources and statistically study the patterns behind the competitive relationships. We find that the two networks exhibit different but complementary patterns of competitions. Our proposed model, Topical Factor Graph Model (TFGM), defines a latent topic layer to bridge the two networks and learns a semi-supervised learning model to classify the relationships between entities (e.g., companies or products). We test the proposed model on two real data sets and the experimental results validate the effectiveness of our model, with an average of +46\% improvement over alternative methods.
Link prediction and recommendation is a fundamental problem in social network analysis. The key challenge of link prediction comes from the sparsity of networks due to the strong disproportion of links that they have potential to form to links that do form. Most previous work tries to solve the problem in single network, few research focus on capturing the general principles of link formation across heterogeneous networks. In this work, we give a formal definition of link recommendation across heterogeneous networks. Then we propose a ranking factor graph model (RFG) for predicting links in social networks, which effectively improves the predictive performance. Motivated by the intuition that people make friends in different networks with similar principles, we find several social patterns that are general across heterogeneous networks. With the general social patterns, we develop a transfer-based RFG model that combines them with network structure information. This model provides us insight into fundamental principles that drive the link formation and network evolution. Finally, we verify the predictive performance of the presented transfer model on 12 pairs of transfer cases. Our experimental results demonstrate that the transfer of general social patterns indeed help the prediction of links.
Wikipedia becomes one of the largest knowledge bases on the Web. It has attracted 513 million page views per day in January 2012. However, one critical issue for Wikipedia is that articles in different language are very unbalanced. For example, the number of articles on Wikipedia in English has reached 3.8 million, while the number of Chinese articles is still less than half million and there are only 217 thousand cross-lingual links between articles of the two languages. On the other hand, there are more than 3.9 million Chinese Wiki articles on Baidu Baike and Hudong.com, two popular encyclopedias in Chinese. One important question is how to link the knowledge entries distributed in different knowledge bases. This will immensely enrich the information in the online knowledge bases and benefit many applications. In this paper, we study the problem of cross-lingual knowledge linking and present a linkage factor graph model. Features are defined according to some interesting observations. Experiments on the Wikipedia data set show that our approach can achieve a high precision of 85.8% with a recall of 88.1%. The approach found 202,141 new cross-lingual links between English Wikipedia and Baidu Baike.
We propose a consumption scheduling mechanism for home area load management in smart grid using integer linear programming (ILP) technique. The aim of the proposed scheduling is to minimise the peak hourly load in order to achieve an optimal (balanced) daily load schedule. The proposed mechanism is able to schedule both the optimal power and the optimal operation time for power-shiftable appliances and time-shiftable appliances respectively according to the power consumption patterns of all the individual appliances. Simulation results based on home and neighbourhood area scenarios have been presented to demonstrate the effectiveness of the proposed technique.
Energy efficiency is the most important concern in mobile embedded system design. The conventional wisdom is that there is a tradeoff between energy efficiency and high-performance techniques, such as prefetching. Thus to reduce energy consumption and save chip area, hardware prefetchers are not implemented in most existing embedded mobile systems. However, modern embedded mobile systems have become increasingly powerful and show a great deal of demand for applying high-performance techniques, such as hardware prefetching, to accelerate applications. In this paper, we study whether it would be beneficial to implement hardware prefetchers in embedded mobile systems. We first demonstrate that: contrary to the conventional wisdom, as technology advances (e.g. from 90nm to 32nm), prefetching starts to become energy-efficient while improving performance. Then, we introduce a general analytical model to identify the conditions for prefetching techniques to achieve energy efficiency. Furthermore, we also introduce a series of models to evaluate the energy efficiency of the prefetcher when specific prefetching features are employed. By using these models, system designers can easily and accurately evaluate the energy efficiency of their designs and make decisions on the deployment of hardware prefetchers.
Conformity is a type of social influence involving a change in opinion or behavior in order to fit in with a group. Employing several social networks as the source for our experimental data, we study how the effect of conformity plays a role in changing users' online behavior. We formally define several major types of conformity in individual, peer, and group levels. We propose Confluence model to formalize the effects of social conformity into a probabilistic model. Confluence can distinguish and quantify the effects of the different types of conformities. To scale up to large social networks, we propose a distributed learning method that can construct the Confluence model efficiently with near-linear speedup. Our experimental results on four different types of large social networks, i.e., Flickr, Gowalla, Weibo and Co-Author, verify the existence of the conformity phenomena. Leveraging the conformity information, Confluence can accurately predict actions of users. Our experiments show that Confluence significantly improves the prediction accuracy by up to 5-10% compared with several alternative methods.
In current embedded mobile systems design, the application processor (AP) is often woken up to service interrupts and user requests. However, this kind of wakeups from sleep is very expensive in terms of battery usage. In the observation that the operating system/driver workloads are very light-weight, in this paper we propose the Offload Co-Processor (OCP) SoC architecture. In the OCP SoC design, when the device is idle, we offload the operating system workloads (mainly interrupt handling workloads) to an ultra-low-power coprocessor. This way, the co-processor would be able to handle most wake-up requests without awakening the heavy-weight AP, thus avoiding the overhead of AP spin-up/down. Using GPS continuous sampling workload as a case study, we show that the proposed OCP SoC design would extend battery life by 3.5 folds.
Extensible Markup Language (XML) has become a widely adopted standard for data representation and exchange. However, its features also introduce significant overhead threatening the performance of modern applications. In this paper, we present a study of XML parsing and determine that memory-side data loading in the parsing stage incurs a significant performance overhead, as much as the computation does. Hence, we propose memory-side acceleration which incorporates of data prefetching techniques, and can be applied on top of computation-side acceleration to speed up the XML data parsing. To this end, we study here the impact of our proposed scheme on the performance and energy consumption and demonstrated how it is capable of improving performance by up to 20 percent as well as produce up to 12.77 percent of energy saving when implemented in 32-nm technology. In addition, we implement a prefetcher on an platform in an effort to evaluate its implementation feasibility in terms of area and energy overhead.
Social influence is the behavioral change of a person because of the perceived relationship with other people, organizations and society in general. Social influence has been a widely accepted phenomenon in social networks for decades. Many applications have been built based around the implicit notation of social influence between people, such as marketing, advertisement and recommendations. With the exponential growth of online social network services such as Facebook and Twitter, social influence can for the first time be measured over a large population. In this tutorial, we survey the research on social influence analysis with a focus on the computational aspects. First, we introduce how to verify the existence of social influence in various social networks. Second, we present computational models for quantifying social influence. Third, we describe how social influence can help real applications. In particular, we will focus on opinion leader finding and influence maximization for viral marketing. Finally, we apply the selected algorithms of social influence analysis on different social network data, such as twitter, arnetminer data, weibo, and slashdot forum.
We study an interesting phenomenon of social influence locality in a large microblogging network, which suggests that users' behaviors are mainly influenced by close friends in their ego networks. We provide a formal definition for the notion of social influence locality and develop two instantiation functions based on pairwise influence and structural diversity. The defined influence locality functions have strong predictive power. Without any additional features, we can obtain a F1-score of 71.65% for predicting users' retweet behaviors by training a logistic regression classifier based on the defined functions. Our analysis also reveals several intriguing discoveries. For example, though the probability of a user retweeting a microblog is positively correlated with the number of friends who have retweeted the microblog, it is surprisingly negatively correlated with the number of connected circles that are formed by those friends.
Instance matching aims to discover the linkage between different descriptions of real objects across heterogeneous data sources. With the rapid development of Semantic Web, especially of the linked data, automatically instance matching has been become the fundamental issue for ontological data sharing and integration. Instances in the ontologies are often in large scale, which contains millions of, or even hundreds of millions objects. Directly applying previous schema level ontology matching methods is infeasible. In this paper, we systematically investigate the characteristics of instance matching, and then propose a scalable and efficient instance matching approach named VMI. VMI generates multiple vectors for different kinds of intained in the ontology instances, and uses a set of inverted indexes based rules to get the primary matching candidates. Then it employs user customized property values to further eliminate the incorrect matchings. Finally the similarities of matching candidates are computed as the integrated vector distances and the matching results are extracted. Experiments on instance track from OAEI 2009 and OAEI 2010 show that the proposed method achieves better effectiveness and efficiency (a speedup of more than 100 times and a bit better performance (+3.0% to 5.0% in terms of F1-score) than top performer RiMOM on most of the datasets). Experiments on Linked MDB and DBpedia show that VMI can obtain comparable results with the SILK system (about 26,000 results with good quality).
With the rapid proliferation of social media, more and more people freely express their opinions (or comments) on news, products, and movies through online services such as forums, discussion groups, and microblogs. Those comments may be concerned with different aspects (topics) of the target Web document (e.g., a news page). It would be interesting to align the social comments to the corresponding subtopics contained in the Web document. In this paper, we propose a novel framework that is able to automatically detect the subtopics from a given Web document, and also align the associated social comments with the detected subtopics. This provides a new view of the Web standard document and its associated user generated content through topics, which facilitates the readers to quickly focus on those hot topics or grasp topics that they are interested in. Extensive experiments show that our proposed framework significantly outperforms the existing state-of-the-art methods in social content alignment.
The fast development of hospital information systems (HIS) produces a large volume of electronic medical records, which provides a comprehensive source for exploratory analysis and statistics to support clinical decision-making. In this paper, we investigate how to utilize the heterogeneous medical records to aid the clinical treatments of diabetes mellitus. Diabetes mellitus, simply diabetes, is a group of metabolic diseases, which is often accompanied with many complications. We propose a Symptom-Diagnosis-Treatment model to mine the diabetes complication patterns and to unveil the latent association mechanism between treatments and symptoms from large volume of electronic medical records. Furthermore, we study the demographic statistics of patient population w.r.t. complication patterns in real data and observe several interesting phenomena. The discovered complication and treatment patterns can help physicians better understand their specialty and learn previous experiences. Our experiments on a collection of one-year diabetes clinical records from a famous geriatric hospital demonstrate the effectiveness of our approaches.
It is often challenging to incorporate users' interactions into a recommendation framework in an online model. In this paper, we propose a novel interactive learning framework to formulate the problem of recommending patent partners into a factor graph model. The framework involves three phases: 1) candidate generation, where we identify the potential set of collaborators; 2) candidate refinement, where a factor graph model is used to adjust the candidate rankings; 3) interactive learning method to efficiently update the existing recommendation model based on inventors' feedback. We evaluate our proposed model on large enterprise patent networks. Experimental results demonstrate that the recommendation accuracy of the proposed model significantly outperforms several baselines methods using content similarity, collaborative filtering and SVM-Rank. We also demonstrate the effectiveness and efficiency of the interactive learning, which performs almost as well as offline re-training, but with only 1 percent of the running time.
The theory of structural holes suggests that individuals would benefit from filling the "holes" (called as structural hole spanners) between people or groups that are otherwise disconnected. A few empirical studies have verified that structural hole spanners play a key role in the information diffusion. However, there is still lack of a principled methodology to detect structural hole spanners from a given social network. In this work, we precisely define the problem of mining top-k structural hole spanners in large-scale social networks and provide an objective (quality) function to formalize the problem. Two instantiation models have been developed to implement the objective function. For the first model, we present an exact algorithm to solve it and prove its convergence. As for the second model, the optimization is proved to be NP-hard, and we design an efficient algorithm with provable approximation guarantees. We test the proposed models on three different networks: Coauthor, Twitter, and Inventor. Our study provides evidence for the theory of structural holes, e.g., 1% of Twitter users who span structural holes control 25% of the information diffusion on Twitter. We compare the proposed models with several alternative methods and the results show that our models clearly outperform the comparison methods. Our experiments also demonstrate that the detected structural hole spanners can help other social network applications, such as community kernel detection and link prediction. To the best of our knowledge, this is the first attempt to address the problem of mining structural hole spanners in large social networks.
We study how links are formed in social networks. In particular, we focus on investigating how a reciprocal (two-way) link, the basic relationship in social networks, is developed from a parasocial (one-way) relationship and how the relationships further develop into triadic closure, one of the fundamental processes of link formation. We first investigate how geographic distance and interactions between users influence the formation of link structure among users. Then we study how social theories including homophily, social balance, and social status are satisfied over networks with parasocial and reciprocal relationships. The study unveils several interesting phenomena. For example, “friend's friend is a friend” indeed exists in the reciprocal relationship network, but does not hold in the parasocial relationship network. We propose a learning framework to formulate the problems of predicting reciprocity and triadic closure into a graphical model. We demonstrate that it is possible to accurately infer 90&percnt; of reciprocal relationships in a Twitter network. The proposed model also achieves better performance (&plus;20--30&percnt; in terms of F1-measure) than several alternative methods for predicting the triadic closure formation.
Online social networks become a bridge to connect our physical daily life and the virtual Web space, which not only provides rich data for mining, but also brings many new challenges. In this paper, we present a novel Social Analytic Engine (SAE) for large online social networks. The key issues we pursue in the analytic engine are concerned with the following problems: 1) at the micro-level, how do people form different types of social ties and how people influence each other? 2) at the meso-level, how do people group into communities? 3) at the macro-level, what are the hottest topics in a social network and how the topics evolve over time? We propose methods to address the above questions. The methods are general and can be applied to various social networking data. We have deployed and validated the proposed analytic engine over multiple different networks and validated the effectiveness and efficiency of the proposed methods.
Automatically discovering cross-lingual links (CLs) between wikis can largely enrich the cross-lingual knowledge and facilitate knowledge sharing across different languages. In most existing approaches for cross-lingual knowledge linking, the seed CLs and the inner link structures are two important factors for finding new CLs. When there are insufficient seed CLs and inner links, discovering new CLs becomes a challenging problem. In this paper, we propose an approach that boosts cross-lingual knowledge linking by concept annotation. Given a small number of seed CLs and inner links, our approach first enriches the inner links in wikis by using concept annotation method, and then predicts new CLs with a regression-based learning model. These two steps mutually reinforce each other, and are executed iteratively to find as many CLs as possible. Experimental results on the English and Chinese Wikipedia data show that the concept annotation can effectively improve the quantity and quality of predicted CLs. With 50,000 seed CLs and 30% of the original inner links in Wikipedia, our approach discovered 171,393 more CLs in four runs when using concept annotation.
In recent years, the Web has evolved from a global information space of linked documents to a space where data are linked as well. The Linking Open Data (LOD) project has enabled a large number of semantic datasets to be published on the Web. Due to the open and distributed nature of the Web, both the schema (ontology classes and properties) and instances of the published datasets may have heterogeneity problems. In this context, the matching of entities from different datasets is important for the integration of information from different data sources. Recently, much work has been conducted on ontology matching to resolve the schema heterogeneity problem in the semantic datasets. However, there is no unified framework for matching both schema entities and instances. This paper presents a unified matching approach to finding equivalent entities in ontologies and LOD datasets on the Web. The approach first combines multiple lexical matching strategies using a novel voting-based aggregation method; then it utilizes the structural information and the already found correspondences to discover additional ones. We evaluated our approach using datasets from both OAEI and LOD. The results show that the voting-based aggregation method provides highly accurate matching results, and that the structural propagation procedure effectively improves the recall of the results.
Ride-sharing systems should combine environmental protection (through a reduction of fossil fuel usage), socialization, and security. Encouraging people to use ride-sharing systems by satisfying their demands for safety, privacy and convenience is challenging. Most previous works on this topic have focused on finding a fixed path between the driver and the riders either based solely on their locations or using social information. The drivers' and riders' lack of options to change or compute the path according to their own preferences and requirements is problematic. With the advancement of mobile social networking technologies, it is necessary to reconsider the principles and desired characteristics of ride-sharing systems. In this paper, we formalized the ride-sharing problem as a multi source-destination path planning problem. An objective function that models different objectives in a unified framework was developed. Moreover, we provide a similarity model, which can reflect the personal preferences of the rides and utilize social media to obtain the current interests of the riders and drivers. The model also allows each driver to generate sub-optimal paths according to his own requirements by suitably adjusting the weights. Two case studies have shown that our system has the potential to find the best possible match and computes the multiple optimal paths against different user-defined objective functions.
Collaborative filtering (CF) is an important and popular technology for recommender systems. However, current CF methods suffer from such problems as data sparsity, recommendation inaccuracy, and big-error in predictions. In this paper, we borrow ideas of object typicality from cognitive psychology and propose a novel typicality-based collaborative filtering recommendation method named TyCo. A distinct feature of typicality-based CF is that it finds "neighbors" of users based on user typicality degrees in user groups (instead of the corated items of users, or common users of items, as in traditional CF). To the best of our knowledge, there has been no prior work on investigating CF recommendation by combining object typicality. TyCo outperforms many CF recommendation methods on recommendation accuracy (in terms of MAE) with an improvement of at least 6.35 percent in Movielens data set, especially with sparse training data (9.89 percent improvement on MAE) and has lower time cost than other CF methods. Further, it can obtain more accurate predictions with less number of big-error predictions.
We study the problem of active learning for networked data, where samples are connected with links and their labels are correlated with each other. We particularly focus on the setting of using the probabilistic graphical model to model the networked data, due to its effectiveness in capturing the dependency between labels of linked samples. We propose a novel idea of connecting the graphical model to the information diffusion process, and precisely define the active learning problem based on the non-progressive diffusion model. We show the NP-hardness of the problem and propose a method called MaxCo to solve it. We derive the lower bound for the optimal solution for the active learning setting, and develop an iterative greedy algorithm with provable approximation guarantees. We also theoretically prove the convergence and correctness of MaxCo. We evaluate MaxCo on four different genres of datasets: Coauthor, Slashdot, Mobile, and Enron. Our experiments show a consistent improvement over other competing approaches.
